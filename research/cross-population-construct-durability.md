# Can One Framework Serve Every Person?

**An Evidence Review With Design Implications**
Zach Gilliland & Robert Jacobson | February 2026

---

## The Question

OpenStrengths measures 36 facets of human strength across six domains. We intend to serve veterans transitioning out of the military, high school students preparing for careers, adults in workforce development programs, and anyone else who wants to understand what's strong in them. If a 45-year-old veteran in Virginia and a 20-year-old student in Kansas City both score high on "Curiosity," that comparison has to mean something. If someone in the United States shares their strengths profile with someone in the UK or Kenya, the language has to point at the same thing.

This is not a settled question for our framework. It is a design question — arguably the most consequential one we face. If constructs hold across populations, we build one framework and adapt items to meet people where they are. If they don't, we need separate frameworks for separate groups, producing results that can't be compared.

Every major personality and strengths assessment has bet on the first answer — one framework, adapted items — and earned that bet through decades of empirical testing. We are making the same bet. But we haven't earned it yet.

This paper reviews the evidence, identifies where it supports our approach, and names the places where our specific framework goes beyond what existing evidence can confirm. We're sharing it openly because the question deserves a thorough answer, and because we want others to be able to check our reasoning.

---

## What the Literature Shows

We looked at how every major personality and strengths framework has approached the cross-population question. The pattern is consistent, and understanding it provides context for our own design decisions.

**The Big Five** uses the same five-factor structure across all populations. The NEO-PI-R has been administered in over 50 cultures, and the five-factor structure was "clearly replicated in most cultures and was recognizable in all" (Costa, Terracciano, & McCrae, 2001). McCrae and Costa's position, supported by decades of cross-cultural and behavior genetic studies, is that the Five-Factor Model is "a biologically based human universal" (McCrae & Costa, 1997). When the instrument needed to reach younger or less-educated respondents, they created the NEO-PI-3 — not by redefining the constructs, but by rewriting items for readability. The same 5 domains, the same 30 facets. Validated in adolescents as young as 12-13 with congruence coefficients ranging from .94 to .98 with the adult structure (McCrae, Costa, & Martin, 2005).

**HEXACO** uses the same six-factor structure across all populations. The HEXACO-100 has been tested across 16 languages, achieving configural and metric invariance — meaning the same factors exist and carry the same meaning in every language tested (Thielmann et al., 2020). When the framework needed to reach younger respondents, they created the HEXACO-MSI for ages 10-14 — same six constructs, adapted items. A 2025 longitudinal study confirmed the six-factor structure held across adolescence with adequate reliability, differentiation, and invariance across time (Journal of Research in Personality, 2025).

**VIA Character Strengths** uses the same 24 character strengths across all populations. Results from 75 nations show "substantial cross-cultural similarity in endorsement of the strengths," with a mean effect size of d = 0.21 — a small difference (VIA Institute, 2024). When the framework needed to reach younger respondents, they created the VIA-Youth for ages 8-17 — same 24 constructs, reworded items. The only structural difference: two versions of the spirituality scale for different developmental stages.

**CliftonStrengths** uses the same 34 talent themes for every person who takes the assessment — student, professional, military, international. The same 177 paired-statement items. Gallup's own data shows that "differences are negligible when CliftonStrengths results are broken down by race, gender, age, and nationality." When they needed to reach children ages 10-14, they created StrengthsExplorer — which did consolidate from 34 themes to 10, the most dramatic construct-level adaptation of any major framework, driven by the developmental limitations of pre-adolescents.

The consistent pattern: **the construct stays the same, the items adapt.** This is the standard practice in psychometric science, supported by decades of replication. We take it seriously as evidence that the approach can work.

### Where Our Framework Differs From These Precedents

The precedent above is real, but it is not our evidence. Several important differences separate our framework from the ones that have earned their cross-population validity:

**Scope and granularity.** The Big Five has 5 broad factors; HEXACO has 6. Their cross-cultural evidence is primarily about broad-factor-level structure — whether the same 5 or 6 dimensions emerge across populations. We have 36 facets nested within 6 domains. Facet-level measurement invariance is substantially harder to achieve and less studied than domain-level invariance (Mõttus et al., 2017). The Big Five replicating across 50 cultures does not tell us that our 36 facets will.

**Novel facets.** The Big Five, HEXACO, and VIA earned their cross-cultural evidence with specific facet structures that have been tested over decades. Several of our facets — Boldness, Purpose, Originality, Foresight, Innovation, Expression — don't map 1:1 to any validated framework's facet structure. They are grounded in related constructs, but as specific facets of a strengths model, they are new. We cannot borrow the cross-cultural precedent of frameworks whose facet structures differ from ours.

**Maturity of evidence.** The Big Five has 40+ years of cross-cultural testing. HEXACO has 20+. The VIA has 15+. We are at the beginning of validation, not the end. Citing their results provides context for our design decisions, but it doesn't substitute for our own data.

None of this invalidates our approach. It means we need to be precise about what the existing evidence can and cannot tell us about our specific framework.

---

## What the Professional Standards Say

The **AERA/APA/NCME Standards for Educational and Psychological Testing** (2014) — the governing standard for psychological measurement in the United States — require evidence that a test measures the same construct across groups when used for cross-group comparisons (Standard 3.2). They require validity evidence, fairness evidence, and documentation of adaptations.

Critically, the Standards do **not** require different constructs for different populations. They require evidence that the same construct is validly measured across populations.

The **ITC Guidelines for Translating and Adapting Tests** (Second Edition, 2017) — the international standard for test adaptation — are built on a foundational principle: "what is assessed should be understood in the same way across language and cultural groups." The 18 guidelines address how to adapt items, verify equivalence, and document the process. The entire framework assumes the construct is held constant while items, administration, and norms are adapted.

Both of these standards point in the same direction: maintain construct definitions, adapt items, verify equivalence empirically. It's worth noting that the Standards require validity evidence *for each intended use and population* — which means citing them as support for our approach also means committing to the specific empirical work they require. The standards are both our justification and our obligation.

---

## The Evidence for Construct Durability

Why would a single construct serve multiple populations? The evidence comes from several threads.

### The Van de Vijver Taxonomy: Three Types of Bias

Van de Vijver and Leung (1997) identified three distinct types of bias in cross-population assessment:

1. **Construct bias**: The construct itself is not identical across groups — it doesn't exist or means something fundamentally different in the other population.
2. **Method bias**: The test administration introduces differences — response styles, social desirability, familiarity with the format.
3. **Item bias** (Differential Item Functioning): Specific items function differently across groups, even though the underlying construct is the same.

This taxonomy matters because it separates two different questions: "Does this construct exist in both populations?" and "Do these specific items work the same way in both populations?" These have different answers and different remedies.

When item bias is detected, the standard remedy is item revision or replacement — not construct redefinition. Construct redefinition is only warranted when evidence shows the construct itself does not exist or functions fundamentally differently in the target population. That distinction will be important for how we interpret our own validation results.

### Measurement Invariance: The Empirical Test

Modern psychometrics tests cross-population construct equivalence through measurement invariance analysis, using multi-group confirmatory factor analysis at four hierarchical levels:

1. **Configural invariance**: Does the same factor structure exist in both groups? (Same constructs)
2. **Metric invariance**: Are the factor loadings equivalent? (Same meaning)
3. **Scalar invariance**: Are the item intercepts equivalent? (Can you compare group means directly?)
4. **Strict invariance**: Are the error variances equivalent? (Most stringent)

Dong and Dumas (2020) conducted the most comprehensive systematic review of measurement invariance for personality measures — 95 studies from 75 peer-reviewed articles. Their findings:

- **Configural invariance** (same constructs exist) was generally supported across cultures, genders, and age groups.
- **Scalar invariance** (needed for direct mean comparisons) was achieved across gender in 44.83% of studies and across age groups in 58.82% — but was essentially never achieved across cultural/ethnic groups.

The key interpretation: failure of scalar invariance does **not** mean the construct is different. It means specific items function somewhat differently across groups, requiring item-level adaptation or population-specific norms. The construct itself — the underlying trait being measured — is typically supported at the configural level. But "typically" is not "always," which is why we plan to test this rather than assume it.

### Domain-Level vs. Facet-Level Invariance

Most of the measurement invariance evidence cited above — and most MI research in personality generally — operates at the broad factor level: the Big Five domains, the HEXACO six, the VIA's higher-order virtues. This is where the strongest cross-population evidence exists.

Facet-level invariance is a different and harder problem. Mõttus et al. (2017) demonstrated that personality traits measured below the facet level (at the "nuance" level of individual items) add meaningful predictive validity beyond broad factors — but this granularity comes with increased complexity for invariance testing. At the facet level, more parameters must be estimated, model fit becomes harder to achieve, and small differences between populations are more likely to emerge as statistically significant non-invariance.

Anglim et al. (2020) showed that facet-level personality measurement substantially improves prediction of well-being outcomes over domain-level measurement alone — supporting our decision to measure at the facet level. But the same granularity that adds predictive power also raises the bar for cross-population equivalence testing. We gain precision at the cost of a harder validation challenge.

This is directly relevant: our framework makes claims at the 36-facet level, not just the 6-domain level. The domain-level precedent from Big Five and HEXACO research gives us reason for optimism about our 6 domains. Whether that optimism extends to all 36 facets — especially our novel ones — is an empirical question we haven't answered yet.

---

## What Breaks — And What Doesn't

### The Outer Boundary: When Constructs Fail

The most dramatic failure of cross-population construct validity comes from Gurven, von Rueden, Massenkoff, Kaplan, and Lero Vie (2013), who tested the Big Five among Tsimane forager-horticulturalists in Bolivia. The five-factor structure did not emerge. Instead, a "Big Two" appeared: prosociality and industriousness. Education and Spanish-language exposure did not improve fit.

This is a genuine case of construct failure. It matters not as a prediction of what will happen with our populations — the Tsimane are a small-scale, non-WEIRD society far outside any population OpenStrengths is designed to serve — but as a proof of principle: construct structures can break. The question is always *under what conditions.*

### Within-US Subcultural Variation: The Relevant Unknown

The Tsimane finding establishes that extreme cultural distance can disrupt construct structure. But the more relevant question for OpenStrengths is whether our target US subpopulations — veterans with combat trauma exposure, at-risk youth in under-resourced schools, adults navigating means-tested public benefit programs — have subcultural experiences that could affect construct meaning at the item or method level.

We don't have a clear answer. This isn't at the Tsimane extreme — our target populations share a broad cultural context with the general US adult samples where most personality research is conducted. But shared nationality does not guarantee shared response patterns. A veteran whose primary adult social context has been military culture may interpret "Leadership" or "Boldness" through a lens that civilian respondents wouldn't. An adolescent in an under-resourced school may have a different relationship to "Achievement" or "Purpose" than a student in a well-funded suburban district. An adult in a workforce development program may respond to "Self-Confidence" items differently than someone who has had continuous employment.

These are not predictions of construct failure. They are acknowledgments that within-US population variation at the item and method level is plausible and testable. This is precisely what DIF analysis is designed to detect. We're naming this as an open question rather than assuming it away.

### Acquiescence and Educational Bias

Research on educational bias tells a subtler story. The Big Five factor structure emerged "in a blurry way" at lower educational levels but with "textbook-like clarity" among the highly educated (Rammstedt, Kemper, & Borg, 2013). The mechanism was **acquiescence bias** — "yea-saying" more frequent among persons with lower education and lower verbal comprehension. Critically, **after correcting for acquiescence bias, the Big Five model held at all educational levels.** The breakdown was a measurement artifact, not the absence of the traits.

This finding is directly relevant to us. Our target populations include adults with lower educational attainment in means-tested programs. The evidence suggests their strengths are structured the same way as everyone else's — but measurement instruments must control for acquiescence bias and reading level to capture that structure accurately. This is what our CEFR-matched reading levels and adaptive item generation are designed to address. Whether they actually succeed at it is something we'll need to test.

---

## When Constructs Actually Do Need to Change

The literature identifies a small number of cases where constructs genuinely differ across populations. Understanding these exceptions helps clarify what we should watch for in our own validation.

### Culture-Specific Constructs

The most cited example is **Interpersonal Relatedness** from the Chinese Personality Assessment Inventory (Cheung et al., 2008). A joint factor analysis with the NEO-PI-R showed that this construct — encompassing reciprocity, face-saving, and harmony maintenance — "did not load on any of the Big Five factors." It is a genuinely distinct dimension not captured by Western frameworks.

But here's the critical nuance: Interpersonal Relatedness is an **additional** construct, not a redefinition of existing ones. The Big Five still function the same way in Chinese populations. There is simply a sixth dimension that Western models miss. Similar findings exist for *amae* in Japan (dependent indulgence in relationships), *simpatia* in Latin American cultures (valuing harmonious social relations), and *ubuntu* in South African cultures (communal personhood).

The pattern: culture-specific constructs operate **alongside** universal ones, not instead of them. This suggests that our 36 facets may not capture every culturally specific strength — but it doesn't necessarily mean that the facets we do measure need to be redefined for different populations. We should remain open to the possibility that important things are missing from our framework, even if what's there holds up.

### The "Only Three Factors" Challenge

De Raad and Peabody (2005) argue that only three personality factors — Extraversion, Agreeableness, and Conscientiousness — are fully replicable across languages. Neuroticism and Openness show weaker cross-cultural replication.

This challenge is real but context-dependent. It comes from **lexical studies** — analyzing personality-descriptive words in different languages. When personality is measured with **questionnaire items** (as OpenStrengths does), the five-factor structure replicates much more robustly because questionnaire items capture behavioral patterns not always reflected in single-word descriptors. The distinction matters: OpenStrengths uses behavioral self-descriptions, not lexical trait adjectives. Whether this advantage holds for our specific items is still an open question.

### The Combined Emic-Etic Consensus

The current consensus in cross-cultural personality research favors what Cheung (2009) calls the **combined emic-etic approach**: universal constructs exist AND some cultures have additional, indigenous constructs. The field does not argue that universal frameworks should be abandoned — it argues that they should be supplemented where needed.

This is the position we're working from: our 36 facets are intended to be universal in scope, and we remain open to supplemental, context-specific items where evidence warrants them. We may also find that some facets don't hold up as well as we expect. We won't know until we test.

---

## How Young Can You Go?

The question of age boundaries is a specific case of cross-population construct validity. Here, the research is more nuanced.

### The Developmental Trajectory

Personality traits do not spring into existence at age 18. They develop gradually, becoming more differentiated and stable with age.

Roberts and DelVecchio (2000) compiled 3,217 test-retest correlation coefficients from 152 longitudinal studies. The trajectory is clear:

| Age Period | Test-Retest Stability (6.7-year interval) |
|---|---|
| Childhood (0-12) | .31 |
| College years (18-22) | .54 |
| Age 30 | .64 |
| Ages 50-70 | .74 (plateau) |

Traits become more stable with age, but they are measurable far earlier than adulthood. The question is: at what age does the adult factor structure emerge?

### Age Thresholds From the Evidence

**Ages 16-17+: Adult constructs with minimal modification.** CliftonStrengths requires a minimum age of 15. By age 16, the Big Five structure is fully adult-like and consistently replicable (Soto, John, Gosling, & Potter, 2011). In an Estonian sample, personality structures in 14-year-olds were "very similar to adults" and "practically indistinguishable from adult personality by age 16" (NEO-PI-3 validation data). Our current minimum of 17 is psychometrically conservative and well-supported.

**Ages 13-15: Same constructs, reworded items.** The VIA-Youth is validated for ages 13-17 using the same 24 character strengths as the adult version, with developmentally appropriate item wording. The NEO-PI-3 was validated in adolescents aged 12-13 with congruence coefficients of .94-.98 against the adult structure (McCrae, Costa, & Martin, 2005). Morizot (2014) confirmed that "a five-factor structure corresponding to the adult Big Five was identified in young adolescents," with configural and metric invariance across age groups. Extending to age 13 with reworded items appears well-supported by precedent, though we'd want our own validation data before making that decision.

**Ages 10-12: Feasible with substantial adaptation.** Soto, John, Gosling, and Potter (2008) showed that after controlling for acquiescence, the Big Five factor structure was recoverable from self-reports starting at age 10. The HEXACO-MSI validates the six-factor structure from age 10. Gallup's StrengthsExplorer serves ages 10-14. However, at this age, acquiescence bias is a major measurement concern, factor differentiation is limited, and some constructs — particularly Openness to Experience — show poor reliability (often around .50). Extending to age 10 is possible but would require its own validation study and likely some facet consolidation.

**Below age 10: Requires fundamental redesign.** Self-report becomes unreliable without specialized methods (e.g., the Berkeley Puppet Interview for ages 5-7). Parent/teacher report becomes the primary data source. Abstract constructs like Wisdom, Perspective, and Foresight are developmentally inappropriate. Openness/Intellect may not emerge as an independent factor. This is below the floor for our current architecture.

### Which Facets Are Most and Least Robust Across Age

Not all 36 facets are equally stable across development. Based on the literature:

**Most accessible to younger ages (10+):**
- Curiosity, Learning (Insight) — directly observable in children
- Imagination, Expression (Creativity) — robust in childhood
- Achievement, Perseverance (Drive) — concrete goal-oriented behaviors
- Kindness, Empathy, Caring, Gratitude (Connection) — observable prosocial behavior

**Most problematic below age 13:**
- Wisdom, Perspective, Foresight, Self-Reflection (Insight) — require abstract thinking and metacognition that develop in Piaget's formal operational stage (11+)
- Innovation (Creativity) — requires understanding of systems and implementation in context
- Leadership, Persuasion (Influence) — adult conception requires organizational and social sophistication
- Justice (Connection) — abstract moral reasoning is still developing (Kohlberg's stages)

### The "Disruption Hypothesis"

Research consistently finds that between ages 12-14, Conscientiousness and Agreeableness temporarily decrease while Neuroticism increases, linked to pubertal development. By late adolescence (16-18), traits resume their trajectory toward maturity. This doesn't mean the constructs are different for adolescents — it means trait levels temporarily dip during puberty. The construct is the same; the measurement captures a developmental moment.

---

## What We Know vs. What We're Extrapolating

Not all 36 facets stand on equal evidentiary footing for cross-population durability. Being honest about this is essential for prioritizing our validation efforts and interpreting our results.

We've tiered our facets by the strength of existing cross-population evidence — not as a judgment of their importance, but as a map of where our confidence is warranted and where it isn't.

### Strong Precedent (~20 facets)

These facets map closely to validated scales in the Big Five, HEXACO, or VIA frameworks that have direct cross-cultural evidence at or near the facet level:

| Facet | Domain | Precedent |
|-------|--------|-----------|
| Curiosity | Insight | Big Five Openness facet; VIA character strength; cross-cultural evidence in 50+ cultures |
| Perspective | Insight | VIA character strength; related to Big Five Openness |
| Learning | Insight | Big Five Openness/Intellect facet; cross-cultural evidence in NEO-PI-R studies |
| Wisdom | Insight | VIA character strength; cross-cultural evidence in 75 nations |
| Self-Reflection | Insight | Related to Big Five Openness and private self-consciousness; studied cross-culturally |
| Imagination | Creativity | Big Five Openness facet (Fantasy/Aesthetics); robust in cross-cultural NEO-PI-R data |
| Achievement | Drive | Big Five Conscientiousness facet (Achievement Striving); extensive cross-cultural MI data |
| Self-Discipline | Drive | Big Five Conscientiousness facet; one of the most robust cross-cultural facets |
| Perseverance | Drive | VIA character strength (Perseverance); Big Five Conscientiousness facet; Grit literature |
| Focus | Drive | Related to Big Five Conscientiousness (Deliberation); attentional control literature |
| Composure | Stability | Big Five Emotional Stability / low Neuroticism; among the most replicated factors |
| Optimism | Stability | Extensively validated cross-culturally (LOT-R); related to Big Five Neuroticism (reversed) |
| Resilience | Stability | Well-validated construct (Connor-Davidson Resilience Scale); cross-cultural evidence |
| Kindness | Connection | VIA character strength; Big Five Agreeableness facet; cross-cultural evidence in 75 nations |
| Empathy | Connection | Big Five Agreeableness facet (Tender-Mindedness); extensive developmental literature |
| Forgiveness | Connection | VIA character strength; cross-cultural evidence available |
| Gratitude | Connection | VIA character strength; cross-cultural evidence in 75 nations |
| Leadership | Influence | VIA character strength; CliftonStrengths theme; cross-cultural leadership literature |
| Communication | Influence | Related to Big Five Extraversion facets; cross-cultural evidence in communication studies |
| Assertiveness | Influence | Big Five Extraversion facet; cross-cultural NEO-PI-R data |

For these facets, we have reasonable grounds to expect cross-population durability based on existing evidence. "Reasonable grounds" is not the same as proof — our specific operationalizations still need their own validation — but the underlying constructs are well-established.

### Moderate Precedent (~10 facets)

These facets are grounded in established constructs but lack direct cross-population measurement invariance data at the facet level, or their mapping to established frameworks is less direct:

| Facet | Domain | Status |
|-------|--------|--------|
| Resourcefulness | Creativity | Related to problem-solving self-efficacy; less studied as a standalone personality facet cross-culturally |
| Flexibility | Creativity | Related to Big Five Openness and cognitive flexibility; invariance data at this specific level is limited |
| Self-Confidence | Drive | Well-studied as self-efficacy (Stajkovic & Luthans, 1998; Scholz et al., 2002 found generalized self-efficacy valid across 25 nations) but our operationalization as a strengths facet is specific to our framework |
| Patience | Stability | Related to Big Five Agreeableness; our operationalization (renamed from Tolerance) is narrower than the Agreeableness domain |
| Adaptability | Stability | Related to HEXACO and Big Five flexibility constructs; growing cross-cultural evidence but less than core factors |
| Self-Regulation | Stability | Well-studied construct, some cross-cultural evidence, but facet-level MI data specifically is limited |
| Caring | Connection | Related to Big Five Agreeableness (nurturance); overlaps with VIA Kindness but operationalized differently |
| Justice | Connection | VIA character strength (Fairness); but abstract moral reasoning is culturally loaded |
| Initiative | Influence | Related to proactive personality literature; less cross-cultural MI evidence than core Big Five facets |
| Persuasion | Influence | Influence and persuasion literature exists; cross-cultural specifics at the facet level are less studied |

For these facets, the underlying psychological constructs are well-established, but the specific cross-population evidence is thinner. They are reasonable candidates for cross-population stability, but they carry more empirical uncertainty than the first tier.

### Limited Precedent (~6 facets)

These facets are novel or recombined in ways that mean we are building the construct more than inheriting it. Cross-population evidence for the specific construct as we've defined it is minimal or nonexistent:

| Facet | Domain | Status |
|-------|--------|--------|
| Originality | Creativity | Our narrowing of the broader "creativity" construct into a specific facet focused on novel idea generation; creativity research exists but our specific operationalization is new |
| Innovation | Creativity | Practical idea implementation as a personality facet (rather than an organizational outcome); limited precedent as a measured individual difference |
| Expression | Creativity | Communicating inner experience through creative means as a standalone facet; novel in strengths frameworks |
| Purpose | Drive | Renamed from Motivation; operationalized as intrinsic alignment between effort, values, and identity — a specific construction without direct facet-level precedent |
| Foresight | Insight | Anticipatory cognition as a standalone strengths facet; related to prospection literature but novel as a measured individual difference in this context |
| Boldness | Influence | Willingness to take calculated risks; related to HEXACO Honesty-Humility (reversed, partially) and sensation-seeking, but novel as a positively-framed strengths facet |

**These are where construct failure is most likely if it occurs.** That doesn't mean they're bad facets — it means they deserve the most rigorous scrutiny in our validation studies, not the least. If our cross-population MI testing reveals problems, we'd expect them to show up here first. We've designed our validation plan with this in mind.

---

## AI-Powered Item Generation: Opportunities and Open Questions

Our item generation pipeline uses large language models to create assessment items from canonical facet definitions, matched to each respondent's reading level and role context, verified through Natural Language Inference (NLI) for semantic alignment. This approach is central to our cross-population strategy: rather than creating static item sets that require manual adaptation for each population, we generate contextually appropriate items from the same construct definitions.

This section treats AI-powered item generation separately from the traditional adaptation evidence discussed above, because it is a fundamentally different approach with its own evidence base — or more accurately, with its own lack of one.

### Opportunities

**Context adaptation at scale.** Traditional cross-population adaptation requires teams of experts to rewrite items for each new population, a process that takes months or years per target group. Our pipeline generates items matched to reading level, role context (student, veteran, workforce), and sensitivity parameters from the same facet definitions. If it works as designed, it dramatically reduces the barrier to serving new populations.

**Construct-seeded generation.** Every item is generated from canonical facet definitions that serve as the single source of construct meaning. The definition stays fixed; the expression varies. This is architecturally aligned with the adapt-items-not-constructs principle from the psychometric literature.

**Provenance and traceability.** Every generated item carries a complete trail from facet definition through generation, NLI verification, and calibration. This transparency exceeds what most traditional item development processes provide.

### Risks That Don't Exist in Traditional Adaptation

**AI cultural and linguistic bias.** Large language models are trained on corpora that reflect specific cultural perspectives — predominantly English-language, Western, educated, and internet-connected. Items generated by these models could carry systematic measurement artifacts that map to the training distribution, not to the construct. Traditional human translators and item writers can be selected for cultural competence with the target population; LLMs cannot be selected in the same way.

**Per-respondent item variation complicates DIF analysis.** If items vary by respondent, traditional DIF analysis becomes harder. What is the reference item set? How do you compare item functioning across groups when no two respondents see identical items? This is a methodological challenge that traditional fixed-form assessments don't face.

**NLI verification has limits.** Our NLI gate confirms that a generated item is semantically aligned with the facet definition. It does not confirm that the item will function equivalently across populations. An item can accurately describe Curiosity and still trigger different response patterns in a veteran vs. a high school student — not because the construct differs, but because the item's surface features interact differently with each group's experience. NLI is a necessary gate, not a sufficient one.

**ITC Guidelines were written for human translators.** We're applying principles designed for human-mediated adaptation (cognitive interviewing, cultural review, back-translation) to a generative AI pipeline. The principles are sound, but their application in this context is assumed, not established. No one has validated that ITC-compliant processes produce equivalent results when the "translator" is an LLM.

### Our Mitigation Plan

We're designing our approach to address these risks, not dismiss them:

1. **Anchor items.** Every facet will include static, traditionally written items that serve as measurement anchors. These undergo standard item development and review. They provide a fixed reference point against which AI-generated items can be compared.

2. **DIF analysis: anchors vs. generated items.** Within the same administration, we'll compare DIF rates between static anchor items and AI-generated items. If generated items show systematically higher DIF, the pipeline needs recalibration — the problem is in the generation, not the constructs.

3. **NLI as necessary-but-not-sufficient.** NLI verification is the first gate, not the last. Items that pass NLI still undergo IRT calibration, DIF analysis, and (for initial populations) cognitive interviewing.

4. **Cognitive interviewing with target populations.** Before large-scale administration with any new population, we'll conduct cognitive interviews to check whether items are interpreted as intended. This is standard ITC-recommended practice, and we're not exempting AI-generated items from it.

This is genuinely new territory. We believe the approach has merit — the theoretical alignment with psychometric adaptation principles is real. We also believe it needs its own evidence base. Analogy to traditional adaptation is where we start, not where we stop.

---

## Validation Design

Our validation plan needs to be concrete enough that a psychometrician can evaluate its feasibility and a critic can tell us where it's insufficient. Here's what we're planning.

### Phase 1 — Pilot (N = 300-500)

**Purpose:** Confirm the 6-domain, 36-facet internal structure holds in a general adult sample before testing cross-population invariance.

- Single-population administration (general US adults)
- Exploratory factor analysis (EFA) → Confirmatory factor analysis (CFA)
- Item quality analysis: factor loadings, item-total correlations, inter-item correlations
- Reliability estimates (coefficient alpha, omega) at both domain and facet level
- Identify poorly functioning items before cross-population testing adds complexity
- **Decision point:** If the 36-facet structure doesn't hold in a general adult sample, cross-population testing is premature. We'd need to revise the model first.

### Phase 2 — Cross-Population Measurement Invariance (N = 200-400 per group)

**Purpose:** Test whether the same factor structure holds across our target populations.

- Multi-group CFA across target populations: general adult, student (ages 17+), veteran, workforce development
- Test invariance hierarchically:
  - **Configural invariance:** Same factor structure across groups
  - **Metric invariance:** Equivalent factor loadings across groups
  - **Scalar invariance:** Equivalent item intercepts across groups (required for mean comparisons)
- DIF analysis at both facet and item level, with particular attention to the "limited precedent" facets identified above
- Population-specific norming where scalar invariance is not achieved

### Phase 3 — AI Item Equivalence (embedded in Phase 2)

**Purpose:** Test whether AI-generated items introduce additional measurement non-equivalence beyond what static items show.

- Within the same administration, respondents receive both anchor (static) items and AI-generated items for each facet
- Compare DIF rates between the two item types
- **Decision point:** If AI-generated items show systematically higher DIF than static anchors across populations, the generation pipeline needs revision. If DIF rates are comparable, the pipeline is functioning as intended.

### Decision Tree

We're committing in advance to how we'll interpret results:

- **If configural invariance holds for all 36 facets →** Proceed to metric and scalar testing. The framework's basic structure is supported.
- **If configural invariance fails for specific facets →** Investigate whether the failure is at the construct level (the facet means something different in that population) or the item level (the items aren't capturing the construct well for that group). Item-level problems call for item revision; construct-level problems call for facet redefinition or removal.
- **If configural invariance fails broadly (>6 facets) →** The one-framework approach needs fundamental revision. We would not proceed to deployment without resolving this.

### What "Adjust Accordingly" Actually Means

If, say, 3 of 36 facets show construct-level failure for a specific population, here are the options we'd consider — in order of preference:

1. **Revise items** for that population, keeping the construct (most problems are at the item level, not the construct level).
2. **Redefine the facet** for specific populations, with documented differences.
3. **Replace the facet** with a more robust construct that captures similar behavioral territory.
4. **Remove the facet** from scoring for that population, with transparency about why.

We would not silently discard the data. Results — including failures — would be published.

### Statistical Feasibility

A 36-facet, 6-domain model has substantial parameter complexity. Multi-group CFA with this many factors requires careful attention to sample size:

- With 4-6 items per facet, the model includes 144-216 items and hundreds of parameters per group
- N = 200+ per group is minimally adequate for configural invariance testing, based on common rules of thumb (5-10 observations per parameter), though these rules are approximate
- Scalar invariance testing — constraining intercepts in addition to loadings — requires larger samples for stable estimation
- Partial invariance approaches (releasing constraints for specific items) may be necessary and are psychometrically defensible
- We plan to work with psychometricians to determine exact sample size requirements based on final model specification, and to consider approaches like parceling or domain-level testing as interim steps if the full 36-facet model proves intractable in early samples

We're not claiming these sample sizes are sufficient for every analysis we'd want to run. We're claiming they're feasible starting points, and that we'll scale our analytic approach to match the data we can realistically collect.

---

## Boundaries and Open Questions

We want to be clear about what we're not claiming.

**We don't claim that precedent from other frameworks validates our specific framework.** The Big Five, HEXACO, VIA, and CliftonStrengths earned their cross-population evidence through decades of testing. We are at the beginning of that process, not the end. Their results inform our design decisions; they don't substitute for our own evidence.

**We don't claim our novel facets have the same evidentiary support as our established ones.** Boldness, Purpose, Originality, Foresight, Innovation, and Expression are grounded in psychological research, but as specific facets of a strengths assessment, they are new. They carry more empirical risk than facets with direct precedent, and we've designed our validation approach to reflect that.

**We don't claim universal construct coverage.** Culture-specific constructs exist — Interpersonal Relatedness in Chinese culture, *amae* in Japanese culture, *ubuntu* in South African culture — that may not be captured by our 36 facets. Our framework is designed to measure universal strengths. It may not measure all strengths that matter in every cultural context. If that turns out to be a real gap for populations we serve, we'll need to address it.

**We don't claim scalar invariance without evidence.** We expect that specific items will function somewhat differently across populations. This is normal and well-documented across every major personality framework. It means we cannot compare absolute scores across populations without establishing measurement equivalence first.

**We don't claim our framework works for children under 10.** Below age 10, self-report reliability degrades, acquiescence bias dominates, and several of our facets require cognitive capacities that haven't fully developed. Extending to younger children would require a fundamentally different assessment architecture.

**We don't claim the same items work for every population.** We claim the same constructs should. The distinction between construct-level and item-level adaptation is well-established in the psychometric literature and is central to our approach. But "should" is doing a lot of work in that sentence — our validation studies will determine whether it actually does.

**We don't claim our AI-generated items are free of bias.** We've designed safeguards, but automated generation is new territory. Until we have DIF data from real administrations across diverse populations, we're treating this as a hypothesis, not a conclusion.

---

## References

AERA, APA, & NCME. (2014). *Standards for Educational and Psychological Testing.* American Educational Research Association.

Anglim, J., Horwood, S., Smillie, L. D., Marrero, R. J., & Wood, J. K. (2020). Predicting psychological and subjective well-being from personality: A meta-analysis. *Psychological Bulletin, 146*(4), 279-323.

Ashton, M. C., & Lee, K. (2007). Empirical, theoretical, and practical advantages of the HEXACO model of personality structure. *Personality and Social Psychology Review, 11*(2), 150-166.

Cheung, F. M., van de Vijver, F. J. R., & Leong, F. T. L. (2011). Toward a new approach to the study of personality in culture. *American Psychologist, 66*(7), 593-603.

Cheung, F. M., et al. (2008). The Chinese Personality Assessment Inventory as a culturally relevant personality measure in applied settings. *Social and Personality Psychology Compass, 2*(1), 74-89.

Costa, P. T., Terracciano, A., & McCrae, R. R. (2001). Gender differences in personality traits across cultures: Robust and surprising findings. *Journal of Personality and Social Psychology, 81*(2), 322-331.

De Raad, B., & Peabody, D. (2005). Cross-culturally recurrent personality factors: Analyses of three factors. *European Journal of Personality, 19*(6), 451-474.

Dong, Y., & Dumas, D. (2020). Are personality measures valid for different populations? A systematic review of measurement invariance across cultures, gender, and age. *Personality and Individual Differences, 160*, 109956.

Garcia, L. F., et al. (2022). Exploring the stability of HEXACO-60 structure and the association of gender, age, and social position with personality traits across 18 countries. *Journal of Personality, 90*(3), 397-414.

Gurven, M., von Rueden, C., Massenkoff, M., Kaplan, H., & Lero Vie, M. (2013). How universal is the Big Five? Testing the five-factor model of personality variation among forager-farmers in the Bolivian Amazon. *Journal of Personality and Social Psychology, 104*(2), 354-370.

Hambleton, R. K., Merenda, P. F., & Spielberger, C. D. (Eds.). (2005). *Adapting Educational and Psychological Tests for Cross-Cultural Assessment.* Lawrence Erlbaum.

International Test Commission. (2017). *ITC Guidelines for Translating and Adapting Tests (Second Edition).*

McCrae, R. R., & Costa, P. T. (1997). Personality trait structure as a human universal. *American Psychologist, 52*(5), 509-516.

McCrae, R. R., Costa, P. T., & Martin, T. A. (2005). The NEO-PI-3: A more readable revised NEO Personality Inventory. *Journal of Personality Assessment, 84*(3), 261-270.

McCrae, R. R., et al. (2005). Universal features of personality traits from the observer's perspective: Data from 50 cultures. *Journal of Personality and Social Psychology, 88*(3), 547-561.

McGrath, R. E. (2015). Measurement invariance in translations of the VIA Inventory of Strengths. *European Journal of Psychological Assessment, 31*(3), 150-157.

Morizot, J. (2014). Construct validity of adolescents' self-reported Big Five personality traits. *Assessment, 21*(5), 580-606.

Mõttus, R., Kandler, C., Bleidorn, W., Riemann, R., & McCrae, R. R. (2017). Personality traits below facets: The consensual validity, longitudinal stability, heritability, and utility of personality nuances. *Journal of Personality and Social Psychology, 112*(3), 474-490.

Park, N., & Peterson, C. (2006). Moral competence and character strengths among adolescents. *Journal of Adolescence, 29*, 891-909.

Rammstedt, B., Kemper, C. J., & Borg, I. (2013). Correcting Big Five personality measurements for acquiescence. *European Journal of Personality, 27*(1), 71-81.

Roberts, B. W., & DelVecchio, W. F. (2000). The rank-order consistency of personality traits from childhood to old age. *Psychological Bulletin, 126*(1), 3-25.

Scholz, U., Doña, B. G., Sud, S., & Schwarzer, R. (2002). Is general self-efficacy a universal construct? Psychometric findings from 25 countries. *European Journal of Psychological Assessment, 18*(3), 242-251.

Soto, C. J., & John, O. P. (2011). Age differences in personality traits from 10 to 65. *Journal of Personality and Social Psychology, 100*(2), 330-348.

Soto, C. J., John, O. P., Gosling, S. D., & Potter, J. (2008). The developmental psychometrics of Big Five self-reports: Acquiescence, factor structure, coherence, and differentiation from ages 10 to 20. *Journal of Personality and Social Psychology, 94*(4), 718-737.

Stajkovic, A. D., & Luthans, F. (1998). Self-efficacy and work-related performance: A meta-analysis. *Psychological Bulletin, 124*(2), 240-261.

Thielmann, I., et al. (2020). The HEXACO-100 across 16 languages: A large-scale test of measurement invariance. *Journal of Personality Assessment, 102*(5), 714-726.

Van de Vijver, F. J. R., & Leung, K. (1997). *Methods and Data Analysis for Cross-Cultural Research.* Sage.

Wood, J. K., Gurven, M., & Goldberg, L. R. (2020). Ubiquitous personality-trait concepts in 13 diverse and isolated languages. *European Journal of Personality, 34*(6), 1004-1017.

---

*This document reflects the research positions of the OpenStrengths founding team as of February 2026. It reviews the evidence for construct durability across populations and age groups, identifies where our specific framework extends beyond existing evidence, and lays out a concrete validation design. For OpenStrengths' design convictions, see the OpenStrengths Manifesto. For complete psychometric specifications, see the OpenStrengths Technical White Paper.*
