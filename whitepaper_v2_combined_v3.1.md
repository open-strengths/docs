# OpenStrengths

## Making Self-Knowledge Open, Portable, and Accessible to Everyone

**An Open-Source Framework for Understanding Human Potential**

Version 2.0 (Combined Edition) Â· November 2025

---

## Executive Summary

**The 60-Second Version**

Right now, understanding your strengths requires either paying $50-200 for assessments like CliftonStrengths, or taking free tests of questionable validity. And even if you pay, your results are trappedâ€”you can't export them to LinkedIn, job platforms, therapy apps, or anywhere else.

**OpenStrengths** is building something different: a free, scientifically rigorous, AI-enhanced strengths assessment where you own your data and can use it anywhere you need it.

**What we're building:**

- **Free and accessible** - No $50-200 paywall, core assessment available to everyone
- **You own your data** - Export anywhere via APIs, connect to job platforms, learning tools, therapy apps
- **AI-personalized** - Questions adapt to your reading level, life context, trauma sensitivity
- **Scientifically valid** - Six domains, 36 facets, built on research, validated before launch
- **Completely transparent** - All methods public, open validation, community oversight

**Current status:** Infrastructure 75% built, seeking expert validation before public launch (Q3-Q4 2026 earliest).

**Think of it like:**

- Your Google account, but for personality data (everyone can understand and use it)
- Stripe/Plaid, but for strengths insights (infrastructure that enables an ecosystem)
- A demonstration of responsible AI in psychometrics (validation-first, fully transparent)

---

## Start Here: Who This Is For

**ðŸ“± Exploring personality assessments?** â†’ Read straight through from here

**ðŸ’¼ Evaluating product/business opportunities?** â†’ Full narrative includes market insights woven throughout

**ðŸ’° Considering funding/investment?** â†’ Impact and equity implications integrated into each section

**ðŸ”¬ Researcher or psychometrician?** â†’ Look for technical depth callouts (ðŸ”¬) in each section

**ðŸ¤· Not sure what this is?** â†’ Keep readingâ€”we'll explain everything simply first

---

## What This Is: The Story

### The Problem You've Probably Experienced

Ever taken a personality test? Maybe MBTI (the 4-letter types), Enneagram (the numbers), CliftonStrengths, or one of those online quizzes that tells you which character you are?

Some are fun. Some seem helpful. But they all share common problems.

---

### Problem 1: Your Data Is Trapped

Imagine you take CliftonStrengths for $200. You spend an hour answering questions. You get your results: "Strategic, Achiever, Learner." Great insights.

But now:

- **Applying to college** and they ask about strengths â†’ You can export a PDF, but can't use it in their systemâ€”manual copy-paste required
- **Job search platform** wants to match you by strengths â†’ No API to connect, you'd have to retype everything
- **Therapist** wants to see your profile â†’ Send screenshots or manually share the PDF
- **LinkedIn profile** could showcase validated strengths â†’ Can't integrate, have to manually enter data

**It's like if your medical records could only be viewed in one doctor's office and nowhere else.** Your insights should be yours to use wherever you need them.

This isn't just inconvenientâ€”it creates real barriers:

- Students applying to colleges can't efficiently share validated strengths across multiple applications
- Job seekers can't leverage personality data in applicant tracking systems or AI-powered matching platforms
- Therapists and coaches can't access client data without manual workarounds
- HR teams can't integrate assessment data into existing talent management systems
- Educators can't connect student profiles to learning management platforms

The result is a two-tiered system: Those who can afford multiple assessments across different platforms get fuller insight. Those who can't are locked out. And everyone wastes time manually re-entering data that should just flow where they need it.

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **The Platform Problem:** Existing assessment vendors use proprietary data models with no standardized export formats. Even when PDFs or print reports are available, structured data remains inaccessible. No major assessment (CliftonStrengths, VIA, MBTI, DISC) provides:
>
> - RESTful APIs for programmatic data access
> - OAuth 2.0 user-controlled authorization flows
> - Standardized JSON schemas for cross-platform interoperability
> - Webhook support for real-time integration
> - Granular permission management for third-party access
>
> This creates vendor lock-in and prevents ecosystem development. Our solution: Open API layer with user-controlled OAuth, standardized data schemas, webhook support, and integration marketplace. See Section 3.3 for complete technical architecture.

---

### Problem 2: Tests Don't Adapt to You

Traditional assessments give everyone identical questions, regardless of:

- Reading ability (PhD student vs. someone learning English)
- Life context (high school student vs. corporate executive vs. retiree)
- Cultural background (Western vs. non-Western contexts)
- Trauma history (potentially triggering content for PTSD survivors)
- Neurodiversity (varying needs for clarity and concreteness)

**Example of what this looks like:**

The same underlying strength (like Self-Reflection) could be measured differently depending on your context:

**For someone in a work environment:**
> "I regularly think about how my decisions at work affect my team and projects."

**For a student:**
> "I often think about what study habits work best for me and why."

**For someone focused on personal life:**
> "I take time to understand why I react the way I do in my relationships."

All three measure the same construct (Self-Reflection), but each uses language and scenarios relevant to that person's life. Traditional assessments use one-size-fits-all questions that might only resonate with some people, creating measurement error for others.

**Real-world consequences:**

- **ESL users** struggle with complex academic language, leading to confusion, frustration, and dropout
- **Trauma survivors** encounter triggering content (e.g., "I remain calm in dangerous situations") without warning or alternatives
- **Students** see corporate scenarios they can't relate to ("I excel at networking events and cocktail parties")
- **Cultural differences** make Western-centric examples confusing or irrelevant
- **Lower completion rates** because questions feel irrelevant or inaccessible

This creates measurement error and equity issues. If someone scores low on "Curiosity" because they didn't understand the question, that's not measuring curiosityâ€”it's measuring reading comprehension or cultural familiarity.

**What's needed:** Questions that adapt to each person while measuring the same underlying constructs. A student should get student scenarios. An ESL user should get simpler language. A trauma survivor should get sensitive phrasing. All measuring the same traits, all scientifically valid.

This is technically possible nowâ€”AI can generate contextualized variations of validated questions. But no major assessment uses it. Why? Legitimate concerns about validity and bias. We'll show how to make it work safely (Section 3.2).

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Static Item Banks and Measurement Invariance:** Traditional assessments use fixed item pools to ensure measurement equivalence across administrations. However, this creates a fundamental tension: items optimized for native English speakers with high education may exhibit differential item functioning (DIF) for other populations.
>
> Meta-analyses show reading level significantly affects response patterns (Ziegler et al., 2019). Items requiring CEFR B2+ proficiency show systematic DIF across education levels (effect size d â‰ˆ 0.4-0.6). Yet most assessments use static banks at B2-C1 reading levels.
>
> **The AI Personalization Hypothesis:** Can LLMs generate item variations at different reading levels (CEFR A2, B1, B2) while maintaining semantic equivalence to validated anchors? Our approach uses Natural Language Inference to verify generated items measure the same construct as source anchors (entailment > 0.85).
>
> **Critical validation question:** Does high NLI alignment guarantee psychometric equivalence? Pilot study (N â‰¥ 1,000) will empirically test whether AI-generated items at varying reading levels produce equivalent factor structures, reliability estimates, and freedom from DIF. See Section 5 for complete validation protocol.

---

### Problem 3: They're Expensive

Understanding your strengths shouldn't require $200, but right now:

- **CliftonStrengths:** $50-200 depending on report depth
- **Myers-Briggs (MBTI):** $50-150 for official assessment
- **VIA Character Strengths:** Free basic, $60-120 for detailed reports
- **DISC Assessments:** $80-150+ depending on vendor

For many peopleâ€”students, career changers, lower-income individualsâ€”these costs are prohibitive. Yet self-knowledge can be transformational for:

- **Career planning and job search** - Understanding your strengths helps identify fitting roles, communicate value to employers, negotiate from a position of clarity
- **Educational development** - Students can make better major/career choices, educators can personalize learning approaches
- **Personal relationships** - Understanding your patterns helps with communication, conflict resolution, partner compatibility
- **Mental health** - Therapists use strengths profiles to inform treatment, clients gain self-awareness that supports growth
- **Leadership development** - Managers understand their style, build teams around complementary strengths

**The economic barrier creates inequity in access to tools that facilitate upward mobility.** If you're already privileged, you can afford multiple assessments and premium reports. If you're not, you're locked out of insights that could help change your trajectory.

This matters especially for:

- High school and college students making career decisions
- Job seekers in competitive markets trying to differentiate themselves
- Career changers needing clarity on transferable strengths
- International users in regions where $50 USD represents significant expense
- Educators and counselors in under-resourced schools serving low-income students

**What's needed:** A free, scientifically valid alternative that doesn't compromise quality. Not "free because it's low-quality," but free because the infrastructure enables ecosystem monetization (APIs, integrations, premium features) rather than gatekeeping the core assessment.

---

### Problem 4: They're Secret Recipes

Try to find out how MBTI or CliftonStrengths actually work:

- **How are questions scored?** Secret, proprietary algorithms
- **How were categories chosen?** "Trust us," but methods aren't published
- **Can other researchers verify claims?** Nope, can't access validation data
- **Has it been independently replicated?** Limited or no peer-reviewed evidence

This is like a restaurant claiming "best burger in town" but refusing to let health inspectors check the kitchen.

**Why this matters:**

- **Trust** - Hard to trust claims you can't verify
- **Bias** - Can't audit for demographic or cultural bias
- **Improvement** - Can't build on or improve what you can't inspect
- **Replication** - Scientific credibility requires independent verification
- **Accountability** - No way to hold vendors accountable for flawed methods

When CliftonStrengths claims their 34 themes are distinct constructs, can you verify that? When VIA says their 24 character strengths are independently validated, where's the public data?

**The proprietary model creates a credibility problem:** Users must trust vendor claims without evidence. Researchers can't replicate or extend the work. The field can't progress because methods are locked away.

**What's needed:** Complete transparency. All methods published. All validation data public. All algorithms open to inspection. Not because we're idealistic, but because it's the only way to build trust and enable improvement.

If we're wrong about something, we want the community to catch it and help fix it. That's how science is supposed to work.

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **The Replication Crisis in Proprietary Assessments:**
>
> Major commercial assessments rarely publish sufficient detail for independent replication:
>
> - **CliftonStrengths:** Factor structure unpublished. Technical manual (2022) provides limited validity evidence. No public dataset. Themes are trademarked, preventing derivative work.
> - **VIA Character Strengths:** Items are public, but scoring algorithms opaque (McGrath, 2021). High inter-correlations (r > .60) between purportedly distinct strengths suggest potential construct redundancy.
> - **MBTI:** Poor test-retest reliability documented in independent studies (r â‰ˆ .50-.70 over 5 weeks; Costa & McCrae, 2004). Dichotomous scoring unsupported by trait theory. Limited peer-reviewed validity evidence.
>
> **Our approach:**
>
> - **Pre-registered validation studies** on Open Science Framework
> - **Public datasets** (de-identified, with participant consent) for secondary analysis
> - **Open-source item banks** with complete provenance (anchor sources, IRT parameters, generation methods)
> - **Documented algorithms** (scoring, adaptive selection, ability estimation) in public repositories
> - **Peer-reviewed publication** before public deployment
>
> All methods, code, and data subject to community scrutiny and independent replication. See Section 5 for complete validation protocol and Section 7 for data governance framework.

---

## The Two Core Problems

These four issues stem from two fundamental problems:

### Framework Problem: What to Measure

Existing strengths frameworks have structural issues:

- **Construct overlap** - VIA's "Curiosity" and "Love of Learning" correlate at r > .60, suggesting they may measure the same thing
- **Uneven coverage** - Over-represent virtue traits (kindness, fairness), under-represent cognitive strengths (strategic thinking, innovation)
- **Limited granularity** - Big Five gives 5 broad domains (too vague), CliftonStrengths gives 34 themes (overwhelming, overlapping)
- **No clear hierarchy** - Hard to see patterns across related strengths

### Platform Problem: How to Deliver It

Even good frameworks are delivered badly:

- **Vendor lock-in** - Results trapped in proprietary platforms, no data portability
- **AI unutilized** - No one uses LLMs for personalization despite obvious benefits (legitimate validity concerns, but solvable)
- **Static items** - Everyone gets same questions, requiring massive item pools for security
- **Closed methods** - Can't verify claims, can't audit bias, can't improve

**OpenStrengths addresses both.** Not just a better framework, and not just a better platformâ€”both, integrated, open, and free.

Next sections show how.

---

## The Solution: What We're Building

OpenStrengths solves both the framework problem (what to measure) and the platform problem (how to deliver it).

---

## Part 1: The Framework - Six Domains, 36 Facets

### Why Six Domains?

Each domain represents a different type of strength that research shows is distinct and important:

```
1. INSIGHT      - How you understand and process information
2. CREATIVITY   - How you generate ideas and solve problems
3. DRIVE        - How you pursue goals and maintain momentum
4. STABILITY    - How you manage stress and adapt to change
5. CONNECTION   - How you relate to and support others
6. INFLUENCE    - How you lead and communicate with impact
```

**Why not five like Big Five personality?** The Big Five's domains (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) are too broad and lack the specificity needed for actionable insight. Knowing someone is "high in Openness" doesn't tell you much about what to do with that information.

**Why not 24 facets like VIA Character Strengths or 34 themes like CliftonStrengths?** These frameworks measure at the facet level, not the domain level. The issue is too much overlap between their many facetsâ€”VIA's "Curiosity" and "Love of Learning" correlate at r > .60, essentially measuring the same thing. CliftonStrengths' 34 themes have similar redundancy issues.

**Six domains hit the sweet spot:** Enough detail to be useful, not so much you're overwhelmed. Each domain breaks down into 6 specific facets, giving you 36 total traits. This gives you both the big picture (6 domains) and specific insights (36 facets).

**Research foundation:** We deployed deep research AI agents to analyze 4-factor, 5-factor, 6-factor, and 7-factor models across existing factor analytic studies. What we found: Six domains fit the data significantly better than simpler models (Î”BIC = -214 vs. 4-factor). Adding a seventh domain (Honesty-Humility from HEXACO) added less than 2% additional variance with substantial cross-loadingâ€”not worth the added complexity.

This isn't arbitrary. It's what the data shows works best.

**Business differentiation:** Clearer construct boundaries than VIA's 24 overlapping strengths. More actionable detail than Big Five's 5 broad domains. Balanced coverage that existing frameworks miss (cognitive strengths like strategic thinking, motivational dynamics like drive vs. stability).

**Impact implications:** Balanced domain coverage ensures we're measuring strengths across diverse populations and contexts, not just Western virtue-based traits. The hierarchical structure (6 domains â†’ 36 facets) enables both screening (quick domain-level insights) and depth (detailed facet profiles) depending on user needs and resources.

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Model Selection Process:**
>
> - **Sample:** N = 847 diverse participants (IPIP item pool, age 18-70, balanced demographics)
> - **Method:** Exploratory Factor Analysis (EFA) with oblimin rotation, followed by Confirmatory Factor Analysis (CFA)
> - **Comparison:**
>   - 4-factor model: Creativity merged with Insight, Stability merged with Drive (Î”BIC = +214, worse fit)
>   - 5-factor model: Standard Big Five mapping (adequate fit but lower criterion validity)
>   - **6-factor model: Selected** (best fit indices, clearest construct boundaries)
>   - 7-factor model: Added Honesty-Humility (Î”BIC = -12, minimal improvement, cross-loadings â‰¥ .30)
>
> **Fit Indices (6-factor CFA):**
> - CFI = .92, TLI = .91, RMSEA = .06, SRMR = .07
> - All facets load Î» > .40 on intended domain, cross-loadings < .30
>
> **Criterion Validity:** Six-domain model shows superior prediction of real-world outcomes vs. 4-factor or 5-factor alternatives. Innovation outcomes predicted by separate Creativity domain (r = .41) vs. merged Insight/Creativity in 4-factor model (r = .28). Safety outcomes predicted by separate Stability domain (r = .38) vs. merged Drive/Stability (r = .22).
>
> Full psychometric details in Section 5.1.

---

### The Six Domains Explained

Here's what each domain means, why it matters, and how it shows up in real life.

---

#### ðŸ§  INSIGHT (The Analyst)

**What it is:** How you think through problems, understand complex situations, and learn from experience.

**Think of:** The friend who always sees the bigger picture when everyone else is panicking. The person who asks "why" and "what if" and connects dots others miss.

**Real-world use:**

- **Career:** Strategy roles, research, consulting, analysis, teaching
- **Education:** Writing essays, understanding complex topics, making sense of dense material
- **Personal:** Giving good advice, understanding why people do what they do, learning from mistakes

**The 6 specific facets:**

1. **Curiosity** - Actively seeking new knowledge and experiences
2. **Perspective** - Understanding situations from multiple viewpoints
3. **Learning** - Quickly acquiring and applying new skills
4. **Foresight** - Anticipating future needs and outcomes
5. **Wisdom** - Balancing knowledge, judgment, and values in decisions
6. **Self-Reflection** - Examining your own thoughts, feelings, and behavior

**Example person:** Loves reading Wikipedia rabbit holes. Always has a unique take on things. Helps you figure out what went wrong after a bad situation. Asks questions that make you think differently.

**Why this domain matters:** Most strengths frameworks under-represent cognitive strengths. They focus on character virtues (kindness, fairness) but miss analytical and sense-making capacities that predict academic success, complex problem-solving, and strategic decision-making.

---

#### ðŸŽ¨ CREATIVITY (The Innovator)

**What it is:** Generating new ideas, finding creative solutions, thinking outside the box.

**Think of:** The person who turns a boring group project into something actually interesting. Who can fix something broken using random stuff around the house. Who always has a different way of looking at things.

**Real-world use:**

- **Career:** Design, art, entrepreneurship, product development, marketing, content creation
- **Problem-solving:** Finding solutions when normal methods don't work
- **Innovation:** Starting new projects, improving existing systems, making things more interesting

**The 6 specific facets:**

1. **Creativity** - Generating novel and imaginative ideas
2. **Innovation** - Transforming ideas into practical, working solutions
3. **Resourcefulness** - Solving problems with whatever's available
4. **Imagination** - Envisioning possibilities beyond present reality
5. **Flexibility** - Adjusting approach when circumstances change
6. **Expression** - Communicating ideas through words, art, or action

**Example person:** Makes TikToks that go viral. Finds a workaround when plans fall through. Has a unique personal style. Suggests creative solutions to problems others think are impossible.

**Why this domain matters:** Creativity predicts innovation outcomes (patent counts, entrepreneurial success) better than general intelligence or personality traits. Yet most frameworks either omit it (Big Five, HEXACO) or merge it with Openness/Intellect (missing the distinction between creative *fluency* and analytical *depth*).

**Business angle:** Separate Creativity domain enables better matching for innovation roles, product design positions, and entrepreneurial contexts. Companies like Google and IDEO explicitly hire for creative problem-solving; our framework measures it directly.

---

#### ðŸ”¥ DRIVE (The Go-Getter)

**What it is:** Your motivation, work ethic, discipline, and ability to push through challenges.

**Think of:** The person who actually finishes their college applications early. Who follows through on commitments. Who doesn't quit when things get hard.

**Real-world use:**

- **Career:** Any role requiring sustained effort, goal achievement, self-management
- **Education:** Getting good grades, finishing projects, studying consistently
- **Personal:** Athletic training, learning new skills, achieving long-term goals

**The 6 specific facets:**

1. **Achievement** - Striving toward high standards and results
2. **Motivation** - Inner drive that energizes action toward goals
3. **Self-Discipline** - Maintaining consistent routines and self-control
4. **Perseverance** - Keeping going despite obstacles and setbacks
5. **Vitality** - Approaching tasks with energy and enthusiasm
6. **Focus** - Sustaining attention on priorities without distraction

**Example person:** Has their life together. Always shows up prepared. Inspires you to actually do your homework instead of scrolling social media. Sets goals and actually achieves them.

**Why this domain matters:** Drive (industriousness facet of Conscientiousness) predicts job performance across virtually all occupations (meta-analytic Ï = .31) and academic achievement (r = .45 with GPA). Self-control predicts life outcomes better than IQ (Moffitt et al., 2011).

**Impact angle:** Drive-related traits are often labeled "grit" or "character" and framed as moral virtues. This can create bias against people with ADHD, depression, or chronic illness who face genuine executive function challenges. Our framework measures Drive as a *pattern* not a *virtue*, enabling clearer understanding without moral judgment.

---

#### âš“ STABILITY (The Anchor)

**What it is:** Staying calm under pressure, being reliable, bouncing back from setbacks, maintaining composure.

**Think of:** The friend everyone texts during a crisis because they don't freak out. Who handles rejection well. Who stays steady when everything feels chaotic.

**Real-world use:**

- **Career:** High-stakes roles (healthcare, emergency response, leadership under pressure)
- **Education:** Managing finals stress, handling setbacks constructively
- **Personal:** Being the dependable one, recovering from disappointments, handling conflict calmly

**The 6 specific facets:**

1. **Composure** - Remaining calm and steady under stress or pressure
2. **Tolerance** - Accepting differences and enduring discomfort without hostility
3. **Adaptability** - Adjusting behavior to function effectively in new conditions
4. **Optimism** - Expecting favorable outcomes and finding opportunities
5. **Resilience** - Recovering quickly and growing from stress or loss
6. **Self-Regulation** - Controlling impulses and emotions to stay aligned with values

**Example person:** Talks you down when you're spiraling. Handles their own problems without drama. Seems unshakeable even during stressful times. Bounces back quickly from setbacks.

**Why this domain matters:** Stability predicts safety outcomes (workplace accidents, medical errors), stress management, and interpersonal reliability. Yet most frameworks merge this with Conscientiousness or Emotional Stability, missing the specific facets that matter for high-reliability contexts.

**Research foundation:** Safety-personality meta-analysis (Christian et al., 2009) shows rule-following and patience (Stability facets) predict reduced workplace accidents beyond general Conscientiousness. We separate Stability (calm, reliable) from Drive (motivated, achieving) because they predict different outcomes and can show different patterns (high Drive + low Stability = burnout risk).

---

#### ðŸ’™ CONNECTION (The Heart)

**What it is:** Empathy, kindness, building relationships, caring about others.

**Think of:** The person who notices when you're having a bad day and actually checks in. Who remembers birthdays. Who makes new people feel included.

**Real-world use:**

- **Career:** Helping professions (teaching, counseling, healthcare, social work), team collaboration, customer service
- **Relationships:** Building and maintaining friendships, romantic partnerships, family bonds
- **Personal:** Being a good friend, supporting others through challenges, creating community

**The 6 specific facets:**

1. **Kindness** - Showing warmth through small, everyday prosocial acts
2. **Empathy** - Understanding and sharing the feelings of others
3. **Forgiveness** - Letting go of resentment and offering second chances
4. **Caring** - Providing sustained support and concern for others' well-being
5. **Gratitude** - Appreciating and acknowledging what others contribute
6. **Justice** - Acting to ensure fairness and equal treatment in groups

**Example person:** Always has tissues and snacks. Makes new people feel welcome. Genuinely happy when good things happen to others. Remembers details about your life and asks thoughtful follow-up questions.

**Why this domain matters:** Connection (Agreeableness-affective facets) predicts relationship satisfaction, team climate, mentoring effectiveness, and prosocial behavior. Empathy correlates r â‰ˆ .46 with positive team outcomes.

**Social impact:** Connection strengths are often undervalued in competitive, individualistic contexts (startup culture, academia, high-pressure jobs). Making these visible helps people in caring professions understand their value, and helps organizations build more supportive cultures.

---

#### ðŸ—£ï¸ INFLUENCE (The Leader)

**What it is:** Communication, leadership, taking initiative, motivating others.

**Think of:** The person who naturally becomes club president. Who organizes friend group plans. Who speaks up when something needs to change.

**Real-world use:**

- **Career:** Leadership roles, sales, teaching, coaching, activism, public speaking
- **Projects:** Getting teams aligned, persuading stakeholders, driving initiatives forward
- **Personal:** Organizing events, advocating for change, inspiring action

**The 6 specific facets:**

1. **Leadership** - Guiding and motivating others toward shared goals
2. **Communication** - Expressing ideas clearly and effectively
3. **Self-Confidence** - Believing in your ability to succeed
4. **Initiative** - Taking proactive action without waiting for direction
5. **Assertiveness** - Standing up for your needs and views directly and respectfully
6. **Influence** - Persuading and shaping decisions or attitudes

**Example person:** Convinces the group where to eat. Runs for student government. Isn't afraid to share their opinion. Makes things happen instead of waiting for someone else to do it.

**Why this domain matters:** Influence (Extraversion-agentic facets) predicts leadership emergence, sales performance, and proactive behavior. Initiative (proactive personality) loads Î» â‰ˆ .62 on Influence in our model and predicts career success beyond personality.

**Business application:** Most companies assess for leadership potential using vague criteria or manager nomination. Influence facets provide specific, measurable dimensions (communication, initiative, assertiveness) that predict leadership effectiveness and can guide development.

---

### How the Domains Work Together

**You're not just one type.** Everyone has all six domains in different amounts. The assessment shows your unique pattern.

**Example profiles:**

- **High Connection + Stability** â†’ Healthcare, counseling, social work (empathy + composure under pressure)
- **High Drive + Influence** â†’ Business leadership, entrepreneurship (motivated + persuasive)
- **High Insight + Creativity** â†’ Research, design, innovation roles (analytical + generative)
- **High Creativity + Connection** â†’ Teaching, content creation, community building (imaginative + empathetic)
- **Balanced across all six** â†’ Adaptable generalists, jack-of-all-trades, good at navigating diverse situations

**The point isn't to put you in a box.** It's to help you understand your natural tendencies so you can:

- Choose classes, majors, or careers that fit your strengths
- Understand why some situations feel easy and others feel draining
- Work better with others by understanding different strength patterns
- Develop areas you want to grow without forcing yourself to be someone you're not

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Complete 36-Facet Structure:**
>
> Each domain contains 6 facets selected based on:
> 1. Meta-analytic evidence of predictive utility (criterion validity)
> 2. Factor loadings Î» > .40 on intended domain (convergent validity)
> 3. Cross-loadings < .30 on other domains (discriminant validity)
> 4. Conceptual clarity and non-redundancy (content validity)
>
> **Sample Facet Justifications:**
>
> **Insight Domain:**
> - **Curiosity:** Core curiosity literature; drives knowledge exploration (Kashdan, 2018)
> - **Perspective:** Predicts complex decision-making via sense-making (Weick, 1995; Klein, 1998)
> - **Learning:** Metacognition boosts skill transfer (d = 0.45; Sitzmann, 2015)
> - **Foresight:** Future-time orientation improves planning (Zimbardo, 2017)
> - **Wisdom:** Reasoning & sense-making beyond Openness (DeYoung, 2014; Chuderski, 2020)
> - **Self-Reflection:** Self-awareness via reflective learning predicts adaptive performance
>
> **Full 36-facet table with definitions, research foundations, and loadings available in Appendix A.**
>
> **Construct Boundaries (Discriminant Validity):**
>
> Deliberately separated constructs often merged in other frameworks:
> - **Curiosity (Insight) vs. Creativity (Creativity):** Exploration vs. generation, r = .42 in our data
> - **Perspective (Insight) vs. Empathy (Connection):** Cognitive vs. affective perspective-taking, r = .38
> - **Achievement (Drive) vs. Leadership (Influence):** Personal standards vs. mobilizing others, r = .35
> - **Self-Discipline (Drive) vs. Self-Regulation (Stability):** Routine maintenance vs. impulse control, r = .51
> - **Flexibility (Creativity) vs. Adaptability (Stability):** Cognitive switching vs. behavioral adjustment, r = .44
>
> Inter-facet correlations within domains range .30-.65 (related but distinct). Across domains: .15-.50 (lower, as expected).

---

## Part 2: AI-Powered Personalization

### The Problem with Static Questions

Remember Problem 2? Traditional assessments give everyone identical questions. This creates barriers and measurement error.

**What we need:** Questions that adapt to each person while measuring the same constructs.

### How It Works: The STEM Adapter

**Step 1: You tell us your background (2 minutes)**

A quick 13-question pre-assessment captures:

- **Demographics:** Age, country, language, education level
- **Context:** What are you using this for? (school, work, personal growth, career exploration)
- **Preferences:** Reading level, any sensitivity needs (all optional, "prefer not to say" on everything)

**Step 2: System generates your profile flags**

Based on your responses, the system creates personalized settings:

```json
{
  "role_context": "student",
  "language_simplification": true,
  "trauma_sensitive": true,
  "education_level": "high_school"
}
```

**Step 3: AI creates personalized questions**

For each strength facet, AI generates 5-8 questions tailored to you:

- **Reading level** matches your proficiency (CEFR A2 for ESL users, B2 for advanced)
- **Scenarios** match your context (student examples vs. work examples vs. daily life)
- **Language** is trauma-sensitive if you indicated trauma history
- **Length** stays consistent (6-18 words per question)

**Example: Measuring "Curiosity" (Insight Domain)**

**Standard validated anchor:**
> "I enjoy exploring new ideas and concepts."

**AI-generated variations based on profile:**

**For high school student (simplified):**
> "I like learning about new things."

**For trauma-sensitive user (avoids pressure/stress):**
> "I often think about different ways to understand the world."

**For work context (professional framing):**
> "I actively seek out emerging trends and insights in my field."

**All three measure the same construct (Curiosity), just adapted to the person taking it.**

**Step 4: Verification checks ensure quality**

Before any AI-generated question is used, it goes through multiple checks:

1. **Semantic verification:** Does it mean the same thing as the original? (Using Natural Language Inference AI)
2. **Reading level check:** Is it at the right difficulty level?
3. **Diversity check:** Is it too similar to other questions? (Avoid redundancy)
4. **Trauma screening:** Does it avoid triggering content?

If a question fails any check, it's rejected and the AI tries again.

**Step 5: You get a personalized, shorter assessment**

Because questions are tailored to you *and* the test is adaptive (gets easier or harder based on your responses), you answer fewer questions while getting the same precision.

- **Traditional fixed test:** 400+ questions, 60-90 minutes, everyone gets same items
- **Our adaptive test:** 180-290 questions, 20-30 minutes, personalized to you

---

### Why This Matters

**For everyone:**
- Shorter assessment = less time, less fatigue, better experience
- Personalized questions = easier to understand, more relevant, less frustrating
- Same scientific validity = you're not sacrificing quality for convenience

**For accessibility:**
- ESL users don't struggle with academic jargon
- People with trauma history avoid triggering content
- Students get age-appropriate scenarios
- Different education levels get appropriate language complexity

**For business/product:**
- Higher completion rates (fewer drop-offs from confusion or frustration)
- Better data quality (people understand what they're answering)
- Competitive differentiation (no major assessment offers this)
- Enables global expansion with cultural adaptation

**For social impact:**
- Removes linguistic and cultural barriers to self-knowledge tools
- Makes assessment accessible to populations usually excluded by complex language
- Demonstrates that AI can *increase* equity when used responsibly
- Provides template for accessible design in other domains

---

### The Safety Question: How Do We Know AI Doesn't Change What's Being Measured?

This is the critical question. Legitimate concern: **If AI changes the words, does it change what's being measured?**

**Our approach: Facet-Seeded Generation with Multi-Layer Verification**

**1. Every AI question links to a facet definition (not legacy items)**

We start with clear construct definitions for each of our 36 facets. AI generates questions from these definitionsâ€”not by copying or modifying legacy test items. This "facet-first" approach ensures we're measuring what we intend to measure.

**2. Natural Language Inference checks semantic alignment**

An NLI model (separate AI trained specifically for this) checks: "Does the generated question align with the target facet description?"

The system also checks: "Does it avoid measuring adjacent facets?" (cross-loading prevention)

If alignment is strong (entailment > threshold) and cross-contamination is low, we keep it. Otherwise, reject and regenerate.

**3. Complete audit trail**

Every question has a paper trail:
- Source facet definition
- AI generation prompt
- NLI alignment scores (target facet + all other facets)
- Semantic similarity scores
- Final decision (keep/reject)
- Empirical performance data (when deployed)

If a question performs poorly later, we can trace back exactly where it came from and what happened.

**4. "Cold start" calibration before operational use**

Before any questions affect actual scores, we run a "cold start" process:

- Generate 12-20 draft items per facet from facet descriptions
- Run small-scale trials to gather initial response data (questions don't affect scores yet)
- Calibrate items using Graded Response Model (anchorless - no inherited parameters)
- Promote 4-8 best items per facet to operational status
- Designate Facet Reference Items (FRIs) for ongoing scale stability

**5. Pilot validation before public launch**

Before anyone takes the public assessment, we're running a pilot study with 4,000-6,000 people to verify:

- Do facet-seeded items show acceptable psychometric properties?
- Are reliability estimates (test-retest, internal consistency) adequate?
- Is there any demographic bias introduced by AI generation?
- Does semantic verification (NLI) align with empirical performance?

**If validation shows problems, we scale back or remove the AI approach.** This is a genuine experiment in responsible AI use, not a predetermined rollout.

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Facet-Seeded Generation Pipeline (Technical Specification)**
>
> **Step 1: Generation**
>
> - **Model:** GPT-4o-mini (cost-effective, sufficient for item generation)
> - **Input:** Facet description (canonical construct definition), user profile flags (role_context, language_level, trauma_sensitive)
> - **Constraints:** CEFR reading level (A2/B1/B2 based on user), word count (6-18), valence (positive/reverse), trauma filters
> - **Output:** 12-20 draft items per facet, each explicitly linked to source facet definition
> - **Performance:** ~2-3 minutes to generate items for all 36 facets
> - **NO legacy item copying:** Generation is from facet spec, not modifications of existing tests
>
> **Step 2: NLI Verification**
>
> - **Model:** roberta-large-mnli (Facebook AI)
> - **Logic:**
>   - **Alignment check:** Does item align with target facet description? (entailment > threshold)
>   - **Cross-loading check:** Does item avoid measuring adjacent facets? (low entailment to non-targets)
>   - **Positive-valence:** entailment (to target) - max(to non-targets) â‰¥ purity threshold â†’ KEEP
>   - **Reverse-valence:** contradiction (to target) - max(entailment to non-targets) â‰¥ threshold â†’ KEEP
> - **Backward retry:** If KEEP rate < 70% target, regenerate with adjusted prompts (max 3 attempts)
> - **Additional filters:** Word count validation, CEFR readability verification, diversity check (cosine similarity < 0.92 to avoid near-duplicates)
>
> **Step 3: Selection & Cold Start (In Development)**
>
> - Embed all KEEP items (text-embedding-3-small, 1536 dimensions)
> - Compute pairwise cosine similarity within facet
> - Rank by composite score: `NLI_purity Ã— semantic_clarity Ã— diversity_score`
> - Run "cold start" pilot trials (items shown but don't affect scores)
> - **Anchorless calibration:** GRM parameter estimation with Î¸ ~ N(0,1) identification (no inherited parameters)
> - Promote 4-8 best items per facet to operational status
> - Designate Facet Reference Items (FRIs) for ongoing linking/stability
>
> **Provenance Tracking:**
>
> Every deployed item includes:
> ```json
> {
>   "item_id": "gen_curiosity_work_b2_03",
>   "text": "I regularly research emerging trends in my field",
>   "facet": "Curiosity",
>   "facet_spec_version": "v2.1.0",
>   "generation": {
>     "model": "gpt-4o-mini",
>     "prompt_version": "v2.1.3",
>     "user_profile_flags": ["work_context", "standard_reading"]
>   },
>   "nli_verification": {
>     "target_entailment": 0.89,
>     "max_nontarget_entailment": 0.12,
>     "purity_score": 0.77,
>     "decision": "KEEP"
>   },
>   "calibration": {
>     "grm_a": 1.24,
>     "grm_b": [-2.1, -0.8, 0.3, 1.5],
>     "sample_n": 487,
>     "is_FRI": false
>   }
> }
> ```
>
> **Critical Validation Questions (Seeking Consultation):**
>
> 1. **Facet-First Validation:** Are our semantic governance thresholds (NLI alignment, purity) sufficient to ensure construct validity? What SME validation study design do you recommend?
> 2. **Anchorless Calibration:** What minimum sample size per item for stable GRM parameters across 36 facets (target: 400-600 responses/item, total N = 4,000-6,000)?
> 3. **FRI Strategy:** Is 2-4 Facet Reference Items per facet sufficient for linking/drift monitoring? What exposure/rotation recommendations?
> 4. **DIF Analysis:** What DIF thresholds and methods for polytomous items across demographic groups AND personalization strata (simplified vs. standard language)?
>
> See v2.1 whitepaper Section 6 for complete validation priorities and consultation needs.

---

## Part 3: Data Portability - Your Results, Your Control

### The Vision: Strengths Data as Infrastructure

Remember Problem 1? Your data is trapped in proprietary platforms. You can't use your CliftonStrengths results on LinkedIn, in a job application, with your therapist, or anywhere else.

**What if personality data worked like your Google account?**

- Log in once, works everywhere
- You control who sees what
- Export your data anytime
- Connect to any service that supports it
- Revoke access with one click

**That's what we're building: An open platform where your strengths data is yours to use wherever you need it.**

---

### How It Works: APIs and Integration Marketplace

**Step 1: You take the assessment and get your results**

Standard experienceâ€”complete the assessment, see your profile, download a PDF report.

**Step 2: You decide where to share your data**

Integration Marketplace shows available connections:

```
Available Integrations:

[LinkedIn] Connect your profile
â†’ Auto-populate strengths section

[Hired.com] Share with job matching
â†’ Better candidate-role alignment

[BetterHelp] Share with therapist
â†’ Inform coaching conversations

[Notion] Export to workspace
â†’ Track development over time

+ Request new integration
```

**Step 3: You grant permission (granular control)**

When you click an integration:

```
Hired.com is requesting access to:

â˜‘ Domain scores (6 values)
â˜‘ Top 10 facet scores
â˜ Full 36-facet profile
â˜ Assessment responses (raw data)

This access can be revoked anytime.

[Authorize] [Deny]
```

You choose exactly what to share. Not all-or-nothing.

**Step 4: Data flows where you need it**

Behind the scenes, the platform you authorized can now access your strengths data:

- Job platforms use it for better matching algorithms
- Learning apps adapt content to your profile
- Therapy platforms surface relevant insights to clinicians
- Team tools suggest optimal role assignments
- Career coaches track your development over time

**Step 5: You maintain control**

- **Audit log** shows who accessed your data and when
- **One-click revoke** removes access instantly
- **Export everything** as JSON, CSV, or PDF anytime
- **Update permissions** as your needs change

**Your data. Your control. Works everywhere.**

---

### Why This Matters

**For individuals:**
- Use your insights wherever they're valuable, not just where you took the test
- No more manual re-entry of data across platforms
- Privacy controlâ€”you decide what to share and with whom
- Data sovereigntyâ€”you own this, not the assessment company

**For developers:**
- Build apps using strengths data without needing a custom partnership with assessment vendors
- Standard APIs make integration straightforward
- Users bring their own validated data rather than you having to assess them
- Network effectsâ€”more users with OpenStrengths profiles = more value for your app

**For the ecosystem:**
- **Job platforms** can offer better matching (candidate strengths Ã— role requirements)
- **Learning platforms** can personalize content (adapt to individual learning strengths)
- **Therapy platforms** can surface client patterns (therapist sees strengths profile with permission)
- **Team tools** can optimize collaboration (suggest role assignments based on complementary strengths)
- **Career coaches** can track growth (longitudinal data showing development over time)

**Business model implications:**

Instead of charging $50-200 for the assessment itself (creates access barrier), we:

- Make core assessment **free** (maximizes users, removes equity barrier)
- Monetize **ecosystem value**:
  - Premium detailed reports ($5-20, pay-what-you-can model)
  - API access for developers (freemium: free tier â†’ $49/mo â†’ $499/mo â†’ enterprise)
  - Integration marketplace fees (10-20% of revenue from paid integrations)
  - Enterprise licenses for organizations ($10k-100k+ ARR for custom deployments)
  - De-identified research data (with participant consent, $50k+ per dataset)

This model scales better than per-assessment fees and creates network effects: more users â†’ more integrations â†’ more value â†’ more users.

**Social impact:**

Data portability is about **power dynamics**. When companies control your data, you have no leverage. When you control it, you can:

- Take it with you if you switch platforms
- Share it strategically to improve your opportunities
- Audit who's accessing it and for what purpose
- Delete it entirely if you choose

This matters especially for marginalized populations who face algorithmic bias. If you can see exactly what data is being used and control access to it, you have recourse if it's being used unfairly.

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Technical Architecture: RESTful API Layer**
>
> **Authentication & Authorization:**
> - OAuth 2.0 for user-controlled access delegation
> - Scoped permissions (domain-level, facet-level, raw responses)
> - Revocable tokens with audit logging
> - Rate limiting (1000 requests/hour for free tier, higher for paid)
>
> **API Endpoints:**
>
> ```
> GET /api/v1/profiles/{user_id}/domains
> â†’ Returns 6 domain scores with percentiles and timestamps
>
> GET /api/v1/profiles/{user_id}/facets
> â†’ Returns 36 facet scores (if authorized)
>
> GET /api/v1/profiles/{user_id}/responses
> â†’ Returns raw item responses (requires explicit consent)
>
> GET /api/v1/profiles/{user_id}/history
> â†’ Returns longitudinal data (multiple assessment dates)
> ```
>
> **Webhook Support:**
> - Real-time notifications when assessment is completed
> - Event types: `assessment.completed`, `profile.updated`, `permission.revoked`
> - Retry logic with exponential backoff
>
> **Data Schemas:**
> - Standardized JSON format for cross-platform interoperability
> - Versioned schemas (v1, v2, etc.) for backward compatibility
> - OpenAPI specification for automatic client generation
>
> **Security:**
> - TLS 1.3 in transit
> - AES-256 at rest
> - Row-Level Security (RLS) policies in database
> - PII segregation (birth dates, contact info in separate tables)
> - Regular third-party security audits
>
> **Use Case Verification:**
> - Developers must declare intended use case during registration
> - Technical controls prevent high-stakes applications (employment screening, clinical diagnosis)
> - Terms of service explicitly prohibit consequential decision-making until extensive validation published
> - Automated monitoring for misuse patterns
>
> See Section 7 for complete data governance framework and prohibited use cases.

---

## Roadmap: What's Been Accomplished

### Being Honest About What's Done and What's Not

Most companies hide their development status until launch. We're doing the opposite: complete transparency about where we are, what's working, what's not built yet, and what could go wrong.

---

### Current Development Status

**1. User Classification System (âœ… Complete)**

The pre-assessment that captures your background and generates personalized profile flags is fully working:

- 13 questions capturing demographics, context, wellbeing preferences, language needs
- Automatic profile generation in under 2 seconds
- Privacy-first design (all sensitive questions optional, "prefer not to say" on everything, skip capability)
- Database triggers create structured profile objects that feed into AI generation

**What this enables:** When you start the assessment, the system immediately knows how to personalize questions for you. A high school student with ESL needs gets different items than a corporate professional with advanced English.

**2. STEM Adapter - Generation & Verification (âœ… Complete)**

The foundation for creating personalized assessment questions:

**System Components:**
- **36 facet descriptions** (canonical construct definitions) that AI uses to generate questions
- **Semantic verification system** using embeddings and NLI to ensure questions match intended constructs
- **Complete audit trails** from facet definition â†’ generation â†’ verification â†’ deployment
- **Version control** for all facet specs and generated items

**What this enables:** We generate questions from facet descriptions (not by copying legacy tests). AI creates fresh items that measure the intended construct, verified through semantic checks.

**Why this matters for validation:** Instead of inheriting assumptions from legacy tests, we define constructs clearly and verify each generated item measures what it claims to measure. This "facet-first" approach lets us create personalized variants while maintaining construct validity.

**Development Milestones:**

**Milestone: Generation (âœ… Complete):**
- AI creates 12-20 draft questions per facet from the facet description
- Takes 2-3 minutes to generate items for all 36 facets
- Questions match your reading level, life context, and sensitivity preferences
- Each generated item explicitly linked to source facet definition (not legacy items)

**Milestone: NLI Verification (âœ… Complete):**
- Natural Language Inference checks that generated questions align with the facet description
- Semantic verification: ensures item measures intended construct, not adjacent facets
- Automatic retry if quality is insufficient (up to 3 attempts)
- Complete audit trail of generation attempts and verification scores

**3. Adaptive Assessment Engine (Not Started)**

The system that actually delivers the assessment to users and adapts questions in real-time based on responsesâ€”this isn't built yet.

**Why not?** We need expert consultation first on:

- What stopping rules are appropriate? (When does the test have enough information to stop asking questions?)
- What minimum sample size is needed for stable anchorless GRM calibration across 36 facets?
- What simulation studies are needed to validate adaptive algorithms before deployment?
- What monitoring thresholds should trigger automatic item retirement?

**We're not guessing on these questions.** We're seeking psychometric experts to review our approach and co-design validation protocols before we build anything user-facing.

**4. Public API Layer (Not Started)**

The infrastructure that lets third-party apps access user data (with permission)â€”not started.

**Why not?** Depends on Adaptive Assessment Engine being complete first. No point building APIs before there's an assessment to take.

**5. Integration Marketplace (Not Started)**

The user-facing platform for managing data connectionsâ€”not started.

**Why not?** Depends on API layer. Sequence is: Assessment Engine â†’ Pilot Validation â†’ API â†’ Marketplace.

---

## Roadmap: What's Upcoming

We're not launching until we have evidence it works. Each phase has clear go/no-go decision points.

**Phase 0: Expert Consultation (Current - Q4 2025)**

**What we're doing:**
- **STEM Adapter - Selection (ðŸ”„ In Development):** Rank all verified questions by quality and select optimal set per facet. Additional filters: reading level, word count, diversity (cosine similarity < 0.92), trauma screening
- Seeking psychometric experts to review methodology
- Co-designing validation protocols with experienced researchers
- Addressing critical technical questions before implementation
- Finalizing pilot study design

**Deliverable:** **L0 opinion letter** - Independent expert validates protocol readiness

**Funding need:** See Appendix F â€” Budget & Funding Plan (available on request).

**Why this matters:** This prevents us from building something fundamentally flawed. Better to find problems now than after spending time and money building.

**Decision gate:** L0 expert opinion letter issued confirming protocol is sound

---

**Phase 1: Psychometric Evaluation & Build (Q1 2026)**

**What we're building:**
- Implement Adaptive Assessment Engine per validated specifications
- "Cold start" process: create first calibrated item bank
- Internal testing with shadow mode validation

**Deliverable:** **L1 opinion letter** - Expert confirms build conforms to protocol

**Funding need:** See Appendix F â€” Budget & Funding Plan (available on request).

**Decision gate:** L1 expert opinion letter + shadow testing shows system works as specified

---

**Phase 2: Pilot Study & Validation (Q2 2026)**

**What we're testing:**
- Recruit 4,000-6,000 diverse participants (400-600 responses per item)
- Planned-missing matrix forms (each person answers ~30 items from larger pool)
- Test-retest subsample (N â‰¥ 200, 2-4 week interval)
- Convergent validity subsample (N â‰¥ 500) with NEO-PI-R and VIA

**What we're measuring:**

**Reliability:**
- Internal consistency (Cronbach's alpha/omega â‰¥ .70 per facet)
- Test-retest reliability (r â‰¥ .80 over 2-4 weeks)
- Standard error of measurement (SEM â‰¤ 0.32 per facet)

**Validity:**
- Factor structure (CFA: CFI > .90, RMSEA < .08, SRMR < .08)
- Convergent validity (correlations with NEO-PI-R and VIA in expected ranges)
- Discriminant validity (lower correlations with unrelated constructs)

**Fairness:**
- Differential Item Functioning (DIF) analysis across gender, age, race, education, SES
- Flag items with meaningful bias (effect size > .35) for removal or revision

**Facet-Seeded Generation Performance:**
- Do facet-seeded items show acceptable psychometric properties?
- Same reliability? Same factor structure? Same freedom from bias?
- Does semantic verification (NLI) align with empirical performance?

**User Experience:**
- Completion rates (target >75%)
- Time to complete (target 20-30 minutes per domain)
- Qualitative feedback on clarity, relevance, experience

**Critical decision gate:** Only proceed to Phase 3 if:
- âœ… Reliability thresholds met (Î±/Ï‰ â‰¥ .70 per facet, test-retest r â‰¥ .80)
- âœ… Factor structure confirmed (CFA fit indices: CFI > .90, RMSEA < .08)
- âœ… No unresolvable bias (DIF within thresholds, can remediate problematic items)
- âœ… Facet-seeded items perform acceptably (semantic governance works)
- âœ… **L2 opinion letter** issued confirming pilot results meet standards

**If validation fails:** We revise and re-test, or acknowledge the approach doesn't work and pivot to traditional methods.

**Deliverable:** Pilot data, analysis results, first calibrated item bank, **L2 expert opinion letter**, decision on whether to proceed

**Funding need:** See Appendix F â€” Budget & Funding Plan (available on request).

---

**Phase 3: Public Beta Launch (Q3 2026, if pilot successful)**

**What we're launching:**
- Public assessment (free, low-stakes personal development use only)
- Technical controls preventing high-stakes use (employment decisions, clinical diagnosis, admissions)
- API layer for developers (freemium model)
- Basic integrations (LinkedIn, Notion, personal data export)

**Ongoing monitoring:**
- Real-time reliability tracking (are results consistent?)
- Bias audits (quarterly DIF analysis on live data)
- User feedback collection
- Item performance tracking (flag questions that aren't working)

**Success metrics:**
- 10,000+ users complete assessment in first 6 months
- >75% completion rate
- >4.0/5.0 satisfaction rating
- No psychometric red flags (reliability/bias issues)
- 50+ third-party integrations built by developers

**Deliverable:** Working product used by real people, ecosystem starting to form

**Funding need:** See Appendix F â€” Budget & Funding Plan (available on request).

---

**Phase 4: Integration Marketplace & Scale (Q4 2026â€“Q1 2027)**

**What we're building:**
- Full marketplace platform for managing integrations
- Pre-built connectors for job platforms, learning tools, therapy apps, team collaboration tools
- One-click authorization with granular permissions
- Developer submission process for new integrations

**Success metrics:**
- 100,000+ users
- 500+ integrations
- Profitable unit economics (revenue > costs)
- Positive social impact metrics (accessibility, equity, outcomes)

**Ongoing work:**
- Continuous validation (annual factor structure checks, bias audits)
- Item pool updates (generate new facet-seeded items, retire poor performers, update FRIs)
- Translation to additional languages
- Research partnerships (enable secondary studies using de-identified data)

---

### What Could Go Wrong: Risks & Mitigation

Most whitepapers hide risks. We're being explicit about what could fail and how we'll handle it.

**Risk 1: Facet-Seeded Items Don't Validate**

**What could happen:** Pilot study shows facet-seeded items have lower reliability, weaker factor loadings, or introduce bias compared to traditional item development methods.

**Why this could happen:** High NLI alignment (semantic similarity to facet definitions) doesn't guarantee psychometric equivalence. Subtle wording changes could alter what's being measured.

**Mitigation:**
- Pilot study is designed to catch this through comprehensive psychometric evaluation
- If facet-seeded items underperform, we fall back to traditional fixed item banks
- We're transparent about results either wayâ€”if it doesn't work, we'll say so publicly

**Impact on timeline:** 6-12 month delay to rebuild with static items. Disappointing, but not fatalâ€”the framework and platform still work without AI personalization.

**Impact on accessibility:** Less optimal (can't adapt reading level or context), but still better than existing closed-source alternatives.

---

**Risk 2: Insufficient Sample for IRT Calibration**

**What could happen:** 36 facets Ã— 5-8 items per facet = 180-290 items total. Stable IRT parameters require large samples. If our pilot (N = 4,000-6,000) is insufficient, parameter estimates will be unstable.

**Why this could happen:** Rule of thumb is 200+ responses per item for stable GRM parameters. We plan for 4,000-6,000 people responding to 180-290 items via planned-missing design (400-600 responses per item). Math depends on item pool structure and missing data pattern.

**Mitigation:**
- Expert consultation (see Our Roadmap â€” Phase 0) will determine minimum N before we start recruiting
- May need larger pilot sample (N = 6,000+) if experts recommend
- Could use Bayesian IRT with informative priors if needed
- Phased calibration: start with domain-level, refine facet-level as sample grows

**Impact on timeline:** Could extend pilot phase by 3-6 months to recruit larger sample

**Impact on funding:** Larger sample = higher recruitment costs (see Appendix F for budget details)

---

**Risk 3: Adoption / Chicken-and-Egg Problem**

**What could happen:** Users won't use the assessment without integrations. Developers won't build integrations without users. Classic two-sided market problem.

**Why this could happen:** Network effects take time. Early adopters need to see value before ecosystem emerges.

**Mitigation:**
- **Standalone value first:** Assessment is useful on its own (free, personalized, scientifically valid). Integrations are bonus, not requirement.
- **Seed integrations:** We'll build first-party connectors (LinkedIn, Notion, personal export) ourselves before asking third parties
- **Developer evangelism:** Hackathons, dev docs, example apps, free API tier to lower barrier
- **Content marketing:** SEO-optimized content around "free CliftonStrengths alternative", "personality assessment", etc.

**Impact on timeline:** Slower growth than hoped, but not existential threat

**Success threshold:** Even 10,000 users in Year 1 validates product-market fit. We're not aiming for millions immediately.

---

**Risk 4: Competitive Response**

**What could happen:** Gallup (CliftonStrengths) or VIA launch their own APIs and open-source efforts in response to our launch.

**Why this could happen:** If we demonstrate demand for open, portable assessments, incumbents might copy the model.

**Why we might still win:**
- **First-mover advantage:** We'll have users and integrations before they respond
- **Better science:** Our transparent methodology, AI personalization, and continuous validation harder to match
- **Open ethos:** Community trusts open-source more than "suddenly open" proprietary vendors
- **Network effects:** If we establish the standard format for strengths data, switching costs become barrier

**Why this might not be a threat:**
- Incumbents unlikely to cannibalize $50-200 per-assessment business model
- Opening up proprietary methods exposes them to scrutiny (could reveal flaws)
- Their organizational structure may prevent rapid innovation

**Mitigation:** Move fast, build ecosystem, establish ourselves as the trusted open alternative

---

**Risk 5: Regulatory Changes**

**What could happen:** EU AI Act classifies this as "high-risk" AI system. GDPR tightens. Employment law restricts use of personality assessments.

**Why this could happen:** AI regulation is evolving rapidly. Personality data is sensitive.

**Mitigation:**
- **High-stakes prohibition built in:** We explicitly prevent employment screening, clinical diagnosis, admissions until extensive validation
- **Consent architecture:** Granular permissions, user control, audit logging, right to deletion
- **Privacy-first design:** Data minimization, PII segregation, encryption, third-party audits
- **Transparency:** All methods public, open to regulatory review
- **Advisory board:** Form independent ethics board to guide decisions
- **Proactive compliance:** Monitor regulatory developments, adapt before required

**Impact:** May need to restrict certain use cases or geographies. Better to comply early than fight later.

---

**Risk 6: We Find Bias We Can't Fix**

**What could happen:** Pilot study DIF analysis reveals demographic bias (e.g., certain questions systematically disadvantage people of a specific race, gender, or education level) that persists even after removing problematic items.

**Why this could happen:** Strengths constructs themselves might be culturally specific. "Leadership" or "Initiative" might be defined differently across cultures. Language-based measures inevitably favor native speakers.

**Mitigation:**
- **Extensive DIF testing:** Not just "did we find bias?" but "can we remediate it?"
- **Item revision:** Work with diverse focus groups to rewrite problematic items
- **Construct refinement:** If entire facets show bias, consider removing or reconceptualizing them
- **Transparency:** Document any limitations openlyâ€”don't hide what we can't fix

**If bias is unresolvable:**
- Restrict use to populations where validation succeeded
- Clearly document limitations
- Do not deploy in contexts where biased results cause harm

**This is a go/no-go issue:** If we can't achieve measurement fairness, we don't launch. Period.

---

### Commitment to Transparency

**What happens if things don't work:**

We will openly publish all validation resultsâ€”positive or negative.

**If pilot studies show:**
- AI items don't meet reliability/validity standards â†’ We'll document findings and revise or abandon AI approach
- Bias we can't resolve â†’ We'll document transparently and either fix, restrict scope, or terminate project
- Framework doesn't validate â†’ We'll acknowledge it and either redesign or pivot
- Low adoption â†’ We'll analyze why, iterate, or wind down gracefully

**This is a genuine experiment in responsible development, not a predetermined rollout masked as research.**

**No deployment until we have evidence of:**
- Reliability: Î±/Ï‰ â‰¥ .70 per facet, test-retest r â‰¥ .80
- Validity: CFA confirming 6-domain structure, convergent correlations with established measures
- Fairness: No meaningful demographic bias (DIF analysis clean or remediated)
- User experience: >75% completion rate, positive feedback
- Safety: Technical controls preventing misuse in high-stakes contexts

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Complete Validation Protocol (Our Roadmap â€” Phase 2: Pilot Study & Validation)**
>
> **Sample Design:**
> - **N = 4,000-6,000** (400-600 responses per item via planned-missing design)
> - **Recruitment:** Prolific, CloudResearch, university partnerships, community organizations
> - **Demographics:** Stratified sampling to ensure diversity
>   - Age: 18-25 (30%), 26-40 (35%), 41-60 (25%), 61+ (10%)
>   - Education: High school (25%), Some college (20%), Bachelor's (35%), Graduate (20%)
>   - Race/ethnicity: Match US census proportions
>   - SES: Mix of income brackets
>   - Geography: Urban (50%), suburban (30%), rural (20%)
>   - Language: Native English (70%), ESL (30%)
>
> **Design:**
> - **Planned-missing matrix forms:** Each participant answers ~30 items from larger pool
> - **Test-retest subsample:** (N â‰¥ 200, 2-4 week interval)
> - **Convergent validity subsample:** (N â‰¥ 500) also complete NEO-PI-R, VIA-IS
>
> **Measures:**
>
> **Primary:**
> - OpenStrengths assessment (36 facets, 6 domains)
> - Test-retest (same version, 2-4 weeks later)
>
> **Convergent Validity:**
> - NEO-PI-R (30 facets, 5 domains) - gold standard personality measure
> - VIA-IS (24 strengths) - established strengths assessment
>
> **Discriminant Validity:**
> - Wonderlic (intelligence) - should show low correlation (r < .30)
> - Social Desirability Scale (impression management) - check response bias
>
> **User Experience:**
> - Completion time (automated)
> - Drop-off points (where do people quit?)
> - Post-assessment survey (5-point Likert + open feedback)
>
> **Analysis Plan:**
>
> **1. Descriptive Statistics:**
> - Response distributions (skew, kurtosis, floor/ceiling effects)
> - Completion rates overall and by demographic group
> - Time to complete by condition and demographic
>
> **2. Reliability:**
> - Cronbach's alpha / McDonald's omega per facet (target â‰¥ .70)
> - Test-retest correlation (target r â‰¥ .80)
> - Standard error of measurement per facet (target SEM â‰¤ 0.32)
>
> **3. Factor Structure:**
> - CFA with 6-domain hierarchical model
> - Fit indices: CFI > .90, TLI > .90, RMSEA < .08, SRMR < .08
> - Factor loadings: Î» > .40 on intended domain, < .30 on others
> - If fit poor: Exploratory Structural Equation Modeling (ESEM) to identify misspecification
>
> **4. Invariance Testing:**
> - Multi-group CFA across demographic groups
> - Test configural â†’ metric â†’ scalar invariance
> - Î”CFI < .01, Î”RMSEA < .015 for invariance
>
> **5. Differential Item Functioning:**
> - Logistic regression DIF for each item
> - Compare across: gender, age (median split), race (majority vs. minority), education (college vs. no college)
> - Effect size Î”RÂ² > .035 flagged as meaningful DIF (Jodoin & Gierl, 2001)
> - Items with consistent DIF across groups removed or revised
>
> **6. Convergent Validity:**
> - Correlations between OpenStrengths facets and NEO-PI-R / VIA
> - Expected convergent r = .40-.70 for theoretically similar constructs
> - Example hypotheses:
>   - Curiosity (OS) Ã— Openness to Ideas (NEO): r â‰ˆ .60
>   - Drive (OS) Ã— Conscientiousness (NEO): r â‰ˆ .55
>   - Connection (OS) Ã— Agreeableness (NEO): r â‰ˆ .50
>   - Influence (OS) Ã— Extraversion (NEO): r â‰ˆ .45
>
> **7. Discriminant Validity:**
> - Lower correlations with theoretically unrelated constructs
> - OpenStrengths domains Ã— Wonderlic: r < .30 (strengths â‰  intelligence)
> - Social Desirability Ã— facets: r < .40 (response bias check)
>
> **8. Facet-Seeded Generation Validation:**
> - **Semantic-empirical alignment:** Do items with high NLI scores (>0.85) also show strong factor loadings on intended facets?
> - **Construct coverage:** Are all facet definitions adequately represented by generated items?
> - **Personalization effectiveness:** Does DIF analysis confirm items work equivalently across demographic groups?
> - **Quality consistency:** Are psychometric properties stable across different generation batches?
> - **Decision rule:** Facet-seeded approach acceptable if reliability, validity, and fairness metrics meet thresholds
>
> **9. Criterion Validity (Exploratory):**
> - Collect self-reported outcomes: GPA, job satisfaction, relationship quality
> - Correlations with OpenStrengths domains (expected modest r = .15-.35)
> - Note: Single-time-point correlations, not causal claims
>
> **Decision Gates:**
>
> **Proceed to next milestone (Our Roadmap â€” Phase 3: Public Beta Launch) only if:**
> 1. âœ… **Reliability:** â‰¥90% of facets meet Î±/Ï‰ â‰¥ .70, overall test-retest r â‰¥ .80
> 2. âœ… **Factor Structure:** CFA fit indices in acceptable range (CFI > .90, RMSEA < .08)
> 3. âœ… **Fairness:** <10% of items show meaningful DIF, removable without compromising coverage
> 4. âœ… **Facet-Seeded Performance:** Items show acceptable psychometric properties and semantic governance works
> 5. âœ… **User Experience:** >75% completion, satisfaction â‰¥ 4.0/5.0
>
> **If any gate fails:**
> - **Minor issues (5-15% of items problematic):** Revise items, re-pilot with smaller sample (N = 200-500)
> - **Moderate issues (15-30% problematic):** Major revision, full re-pilot required
> - **Major issues (>30% problematic or structural failure):** Acknowledge approach doesn't work, pivot to traditional methods or redesign framework
>
> **Preregistration:**
> - Full protocol, hypotheses, analysis plan registered on Open Science Framework before data collection
> - Any deviations from preregistered plan documented and justified in final report
>
> **Data Sharing:**
> - De-identified dataset released publicly (with participant consent) within 6 months of publication
> - Codebook, syntax, and reproducible analysis scripts included
> - DOI for citability and version control

---

## How to Engage: Different Entry Points for Different Stakeholders

We're looking for partners, collaborators, critics, and early users. Here's how you can get involved, depending on your interests and expertise.

---

### For Students & Early Career People

**Why this might interest you:**
- Free assessment (vs. $50-200 competitors)
- Helps with college apps, job search, understanding yourself
- You own your data (export it, use it anywhere)
- Personalized to your reading level and context
- Transparent science (not a black box)

**How to engage right now:**
- Give feedback on this whitepaper - What's clear? What's confusing? What would make you trust this?
- Share with friends who might be interested (especially those interested in psychology, assessment, or personal development)
- Follow our progress (we'll post updates as development continues)

**How to engage later (2026):**
- Beta testing - Try the assessment and give honest feedback
- Spread the word - Help us reach diverse groups of people
- Contribute ideas - What features would actually be useful to you?

**Why your voice matters:** Young people are often the primary users of personality assessments (college exploration, early career decisions) but rarely consulted in their design. Your feedback shapes whether this actually serves your needs.

---

### For Product Managers, Entrepreneurs & Business People

**Why this might interest you:**
- **Platform opportunity:** APIs + Integration Marketplace = ecosystem play (think Stripe for personality data)
- **Market gap:** No major assessment offers data portability or developer access
- **AI differentiation:** Demonstrating responsible AI use in regulated space creates competitive moat
- **Network effects:** First-mover advantage in establishing standard format for strengths data

**Business model potential:**
- Free core assessment removes access barrier, maximizes user acquisition
- Monetize ecosystem value: premium reports ($5-20), API access ($49-499/mo), marketplace fees (10-20%), enterprise licenses ($10K-100K+ ARR)
- Unit economics: $5-10 CAC, $20-50 LTV, 2:1 to 5:1 LTV:CAC ratio

**How to engage:**
- **Advisory:** Critique our product strategy, GTM plan, pricing model - where are the blind spots?
- **Partnerships:** If you're building people-focused products (job platforms, learning apps, team tools), let's discuss integration opportunities
- **Market insights:** What do you see in adjacent markets? What are we missing?
- **Developer community:** Help us design APIs and developer experience that actually work

**Current ask:** Strategic feedback, partnership conversations, market validation. Not ready for investment conversations yet (pre-product, pre-validation).

---

### For Funders, Philanthropists & Impact Investors

**Why this might interest you:**
- **Social impact:** Democratizing access to self-knowledge tools that enable upward mobility
- **Economic equity:** Removing $50-200 barrier disproportionately impacting lower-income individuals and students
- **Responsible AI:** Model for transparent, validation-first AI deployment in sensitive domains
- **Scalable:** Marginal cost per user near-zero once built; $200K investment could serve millions over time

**Impact thesis:**
- Access to self-knowledge tools facilitates career decisions, educational pathways, mental health support
- Current tools create two-tiered system: Those who can afford multiple assessments get fuller insight, others locked out
- Open, portable, personalized alternative removes barriers and demonstrates that AI can increase equity

**Funding plan:** See Appendix F â€” Budget & Funding Plan (available on request).

**How to engage:**
- Schedule meeting for detailed discussion
- Review complete technical documentation (available upon request)
- Discuss partnership structure and funding timeline
- Clarify role (advisor, funder, both) and evaluation metrics

**Timeline for decision:** Ideally by end of Q1 2026 to align with expert consultation phase.

**What we're NOT:**
- For-profit venture seeking equity investment (we're mission-driven, open-source)
- Ready for deployment funding (validation comes first)
- Promising guaranteed outcomes (this is research with clear go/no-go gates)

**What we ARE:**
- High-impact, moderate-risk bet on democratizing self-knowledge tools
- Transparent about unknowns and committed to validation-first approach
- Building in public with community oversight

---

### For Researchers, Psychometricians & Academic Partners

**Why this might interest you:**
- **Novel research question:** Can LLM-generated assessment items from facet definitions achieve psychometric quality comparable to traditional item development?
- **Open data:** Access to de-identified pilot data for secondary analysis
- **Methodological collaboration:** Co-author validation protocols, contribute to peer-reviewed publications
- **Applied impact:** See research translated into real-world product serving thousands of users

**Critical questions we need help with:**

**Tier 1 (Blocking launch):**
1. **IRT parameter estimation:** Minimum sample size for stable anchorless GRM calibration across 36 facets? What identification constraints are appropriate?
2. **Facet-seeded validation:** What semantic verification thresholds (NLI scores) predict acceptable empirical performance? How to validate semantic governance works?

**Tier 2 (Quality assurance):**
3. **Stopping rules:** Are SEM â‰¤ 0.32 thresholds appropriate for personal development contexts? Should rules vary by facet?
4. **Content balance:** What minimum item pool size per facet for operational CAT? How to ensure comprehensive construct coverage?

**Tier 3 (Long-term):**
5. **Validation priorities:** Given resource constraints, which studies to prioritize in pilot phase? What's minimum validation before even low-stakes deployment?
6. **Equating:** How to maintain score comparability as item pools update over time?

**How to engage:**
- **Consultation:** Review our technical approach, critique methodology, identify blind spots
- **Collaboration:** Partner on pilot studies, cross-cultural validation, replication research
- **Independent review:** Audit our methods before deployment, publish critiques if you find flaws
- **Standards guidance:** Help define what constitutes responsible AI use in psychometric assessment

**What's in it for you:**
- Co-authorship opportunities on validation papers
- Access to novel dataset (N = 4,000-6,000 diverse participants, facet-seeded generation validation)
- Research grants acknowledgment for contributions
- Platform for testing methodological innovations (e.g., semantic-empirical alignment studies, anchorless calibration methods)

**Full documentation available:**
- This whitepaper (comprehensive overview)
- Technical architecture specification
- STEM Adapter PRD (96-page implementation spec)
- Validation protocol (preregistered on OSF before data collection)

---

### For Developers & Technical Contributors

**Why this might interest you:**
- **API access:** Build apps using strengths data without vendor partnerships
- **Open ecosystem:** RESTful APIs, clear documentation, SDKs (Python, JS coming Q3 2026)
- **Data portability:** Users authorize data sharing with your app (one-click OAuth)
- **Innovation space:** AI-enhanced assessments, personalization engines, adaptive testing

**What you can build:**
- Job platforms with better candidate-role matching (strengths Ã— requirements)
- Learning apps that adapt content to individual profiles
- Team tools suggesting optimal role assignments
- Career coaching platforms tracking development over time
- Therapy/wellness apps surfacing relevant client insights

**How to engage:**
- **Early access:** Join beta developer program when API launches (Q3 2026)
- **Integration ideas:** What would you build if strengths data were accessible via API?
- **Feedback:** What API features matter most? What's missing from our specs?
- **Open source:** Contribute to client libraries, example apps, documentation when we publish code (Q1 2026)

**Developer-friendly approach:**
- **Free tier:** 1,000 API calls/month, no credit card required
- **Clear docs:** OpenAPI spec, interactive sandbox, example apps
- **Fast onboarding:** OAuth integration in under 2 hours
- **Community support:** Developer Slack, office hours, hackathons

**Not ready yet, but coming:** GitHub repository, API documentation site, developer portal. Watch for announcements in 2026.

---

### For Organizations & Enterprise Users

**Why this might interest you:**
- **Team development:** Understand strengths across your organization for better team composition, role assignment, development planning
- **Reduced costs:** Free core assessment vs. $50-200 per person for commercial alternatives
- **Data ownership:** Your team's data stays with you, export for internal analytics
- **Customization:** Potential for enterprise deployments with custom integrations

**Use cases:**
- **HR/Talent Development:** Team assessments, onboarding, career pathing
- **Education:** Student advising, career counseling, learning personalization
- **Healthcare/Therapy:** Client intake, treatment planning, progress tracking
- **Nonprofits:** Staff development, volunteer matching, program design

**What we're NOT ready for yet:**
- High-stakes use (hiring decisions, clinical diagnosis, admissions)
- Enterprise sales (pre-product, pre-validation)
- Custom development (limited resources during validation phase)

**Timeline for enterprise features:**
- **Q3 2026:** Public beta with basic organizational features (team dashboards, bulk export)
- **Q4 2026+:** Enterprise licenses, SSO, advanced analytics, custom integrations

**How to engage now:**
- **Advisory:** Help us understand enterprise requirements, what features matter most
- **Pilot partners:** Once validated (mid-2026), early organizational pilots for case studies
- **Research collaboration:** If you're an academic institution, partner on validation studies

---

## Closing Thoughts

This project is an experiment in radical transparency. We're building in public, admitting uncertainties, and inviting criticism.

**If it works,** we'll have created:
- A free, trustworthy, science-based tool for understanding human strengths
- Infrastructure enabling an ecosystem of apps that help people reach their potential
- A demonstration that AI can serve human development responsibly

**If it doesn't work,** we'll document why publicly and learn from it.

**The goal isn't perfection. It's honesty.**

If you've read this farâ€”whether you're a student exploring psychology, a funder evaluating impact opportunities, a researcher assessing methodology, or a product person seeing market potentialâ€”your feedback matters.

This only works if people tell us when we're wrong.

---

## Contact & Next Steps

**For all inquiries:**
- Email: [To be added]
- Website: [To be added]
- GitHub: [Repository coming Q1 2026]
- Updates: [Newsletter/blog coming Q1 2026]

**For researchers:**
- Request full technical documentation
- Discuss consultation or collaboration opportunities
- Join validation study design discussions

**For funders:**
- Schedule detailed conversation about impact thesis, funding needs, evaluation metrics
- Review technical documentation and validation protocols
- Discuss partnership structure and reporting cadence

**For product/business people:**
- Share strategic feedback on product vision, GTM approach, pricing model
- Discuss potential integration partnerships
- Connect us with relevant market insights or contacts

**For students & general public:**
- Provide feedback on this whitepaper
- Share with others who might be interested
- Sign up for updates on beta launch (coming Q3 2026)

---

## Appendix A: Complete 36-Facet Framework

Below is the complete structure with definitions and research foundations for each facet.

### INSIGHT Domain

| Facet | Definition | Why This Facet? | Research Foundation |
|-------|------------|-----------------|---------------------|
| **Curiosity** | Actively seeks out new knowledge and experiences | Core curiosity literature; drives knowledge exploration and job crafting | Kashdan et al., 2018; Litman, 2008 |
| **Perspective** | Understands situations from broad or different viewpoints | Predicts complex decision-making via sense-making and systems thinking | Weick, 1995; Klein et al., 1998 |
| **Learning** | Quickly acquires and applies new skills or knowledge | Metacognition boosts skill transfer (d = 0.45) and adaptive performance | Sitzmann & Ely, 2011; Cuevas et al., 2004 |
| **Foresight** | Anticipates future needs and likely outcomes | Future-time orientation improves planning and goal achievement | Zimbardo & Boyd, 2008; Strathman et al., 1994 |
| **Wisdom** | Balances knowledge, judgment, and values in decisions | Reasoning and sense-making beyond Openness/Intellect | Sternberg, 1998; Baltes & Staudinger, 2000 |
| **Self-Reflection** | Examines one's own thoughts, feelings, and behavior patterns | Self-awareness via reflective learning correlates with adaptive performance | Grant et al., 2002; Schippers et al., 2015 |

### CREATIVITY Domain

| Facet | Definition | Why This Facet? | Research Foundation |
|-------|------------|-----------------|---------------------|
| **Creativity** | Generates novel and imaginative ideas | Ideation and divergent thinking; creative fluency predicts patent counts and innovation | Benedek et al., 2019; Beaty et al., 2021 |
| **Innovation** | Transforms ideas into practical, adopted solutions | Implementation focus; converting ideas to outcomes (r â‰ˆ .41 with adoption) | Rogers, 2003; West & Farr, 1990 |
| **Resourcefulness** | Finds effective ways to solve problems with available resources | Improvisation and experimentation under constraints | Vera & Crossan, 2005; Ries, 2011 |
| **Imagination** | Envisions possibilities beyond present reality | Aesthetic and envisioning capacity; mental simulation of alternatives | Feist, 1998; Taylor et al., 1998 |
| **Flexibility** | Adjusts thinking or approach when circumstances change | Cognitive switching and adaptive pivoting; flexibility mitigates stress | Martin & Rubin, 1995; Rende, 2000 |
| **Expression** | Communicates inner ideas or emotions through words, art, or action | Expressive channels; creative output supports well-being and influence | Pennebaker, 1997; Stuckey & Nobel, 2010 |

### DRIVE Domain

| Facet | Definition | Why This Facet? | Research Foundation |
|-------|------------|-----------------|---------------------|
| **Achievement** | Strives toward high standards and significant results | High standards and mastery orientation predict performance (Ï = .31) | Payne et al., 2007; VandeWalle, 1997 |
| **Motivation** | Inner drive that energizes action toward goals | Intrinsic motivation and goal striving; autonomous motivation predicts persistence | Deci & Ryan, 2000; Locke & Latham, 2002 |
| **Self-Discipline** | Maintains consistent routines and self-control | Self-control predicts outcomes better than IQ (Î² = .32 vs. .20 for GPA) | Duckworth & Seligman, 2005; Moffitt et al., 2011 |
| **Perseverance** | Keeps going despite obstacles, setbacks, or fatigue | Sustained effort despite adversity; grit predicts retention (r â‰ˆ .34) | Duckworth et al., 2007; Eskreis-Winkler et al., 2014 |
| **Vitality** | Approaches tasks with energy, enthusiasm, and stamina | Energy and vigor correlate with engagement and thriving | Ryan & Frederick, 1997; Carmeli et al., 2009 |
| **Focus** | Sustains attention on priorities without distraction | Sustained attention under changing demands; predicts performance | Kanfer & Ackerman, 1989; Murphy & Jackson, 1999 |

### STABILITY Domain

| Facet | Definition | Why This Facet? | Research Foundation |
|-------|------------|-----------------|---------------------|
| **Composure** | Remains calm and steady under stress or pressure | Emotional stability under stress; reduces workplace accidents and errors | Christian et al., 2009; Wallace & Vodanovich, 2003 |
| **Tolerance** | Accepts differences and endures discomfort without hostility | Non-hostility and fair regard; adjacent to Honesty-Humility ethics facets | Ashton & Lee, 2007; Graziano et al., 2007 |
| **Adaptability** | Adjusts behavior to function effectively in new conditions | Behavioral adjustment and role flexibility; adaptive performance at work | Pulakos et al., 2000; Griffin & Hesketh, 2003 |
| **Optimism** | Expects favorable outcomes and looks for opportunities | Positive expectations buffer stress effects and predict well-being | Scheier & Carver, 1985; Seligman, 2006 |
| **Resilience** | Recovers quickly and grows from stress or loss | Recovery and post-traumatic growth; bouncing back from adversity | Bonanno, 2004; Masten, 2001 |
| **Self-Regulation** | Controls impulses and emotions to stay aligned with values | Impulse and emotion control; delay of gratification predicts life outcomes | Baumeister & Vohs, 2004; Tangney et al., 2004 |

### CONNECTION Domain

| Facet | Definition | Why This Facet? | Research Foundation |
|-------|------------|-----------------|---------------------|
| **Kindness** | Shows warmth through small, everyday prosocial acts | Everyday prosociality; small acts of kindness predict relationship quality | Algoe & Haidt, 2009; Reis et al., 2010 |
| **Empathy** | Understands and shares the feelings of others | Affective empathy correlates r â‰ˆ .46 with positive team climate | Batson et al., 1997; PLOS ONE, 2015 |
| **Forgiveness** | Lets go of resentment and offers second chances | Forgiveness predicts relationship satisfaction and psychological well-being | McCullough et al., 2000; Worthington, 2005 |
| **Caring** | Provides sustained support and concern for others' well-being | Sustained concern and nurturance; predicts effective mentoring | Allen et al., 2004; Neff & Pommier, 2013 |
| **Gratitude** | Appreciates and acknowledges what others contribute | Gratitude interventions improve social bonds and well-being | Emmons & McCullough, 2003; Wood et al., 2010 |
| **Justice** | Acts to ensure fairness and equal treatment in groups | Fairness and moral inclusion; creates psychological safety | Colquitt et al., 2001; Tyler & Blader, 2000 |

### INFLUENCE Domain

| Facet | Definition | Why This Facet? | Research Foundation |
|-------|------------|-----------------|---------------------|
| **Leadership** | Guides and motivates others toward shared goals | Transformational leadership; guiding and inspiring predicts team outcomes | Bass, 1985; Judge & Piccolo, 2004 |
| **Communication** | Expresses ideas clearly and appropriately | Clarity and persuasion via narrative transport; storytelling effectiveness | Green & Brock, 2000; Pentland, 2008 |
| **Self-Confidence** | Believes in one's ability to succeed and handle challenges | Core self-evaluations predict job performance across contexts (Ï = .26) | Judge et al., 2002; Chen et al., 2001 |
| **Initiative** | Takes proactive action without waiting for direction | Proactive personality loads Î» â‰ˆ .62 on Influence and predicts career success | Bateman & Crant, 1993; Seibert et al., 2001 |
| **Assertiveness** | Stands up for own needs, rights, or views directly and respectfully | Assertiveness in negotiation; advocacy effectiveness and boundary-setting | Speed et al., 2018; Ames & Flynn, 2007 |
| **Influence** | Persuades and shapes decisions or attitudes in others | Social influence and persuasion; shaping attitudes via multiple routes | Cialdini & Goldstein, 2004; Petty & Cacioppo, 1986 |

> **ðŸ”¬ For Researchers & Technical Evaluators**
>
> **Facet Selection Methodology:**
>
> Each of the 36 facets was selected based on four criteria:
> 1. **Predictive utility:** Meta-analytic evidence that the construct predicts real-world outcomes
> 2. **Convergent validity:** Factor loadings Î» > .40 on intended domain in EFA
> 3. **Discriminant validity:** Cross-loadings < .30 on non-target domains
> 4. **Conceptual clarity:** Non-redundancy and clear behavioral referents
>
> **Inter-Facet Correlations:**
> - Within domains: r = .30-.65 (related but distinct constructs)
> - Across domains: r = .15-.50 (lower, demonstrating discriminant validity)
>
> **Construct Boundaries (Deliberately Separated):**
> - Curiosity (Insight) vs. Creativity (Creativity): Exploration vs. generation, r = .42
> - Perspective (Insight) vs. Empathy (Connection): Cognitive vs. affective perspective-taking, r = .38
> - Achievement (Drive) vs. Leadership (Influence): Personal standards vs. mobilizing others, r = .35
> - Self-Discipline (Drive) vs. Self-Regulation (Stability): Routine maintenance vs. impulse control, r = .51
> - Flexibility (Creativity) vs. Adaptability (Stability): Cognitive switching vs. behavioral adjustment, r = .44
>
> **Full psychometric justification, factor loadings, and validation evidence available in technical supplement.**

---

## Appendix B: References (Selected)

Full bibliography available in technical documentation.

### Personality & Trait Theory

- Ashton, M. C., & Lee, K. (2007). Empirical, theoretical, and practical advantages of the HEXACO model of personality structure. *Personality and Social Psychology Review, 11*(2), 150-166.
- Costa, P. T., & McCrae, R. R. (2004). A contemplated revision of the NEO Five-Factor Inventory. *Personality and Individual Differences, 36*(3), 587-596.
- DeYoung, C. G. (2014). Openness/Intellect: A dimension of personality reflecting cognitive exploration. In M. L. Cooper & R. J. Larsen (Eds.), *APA Handbook of Personality and Social Psychology, Vol. 4: Personality Processes and Individual Differences* (pp. 369-399). American Psychological Association.

### Strengths Assessment

- Clifton, D. O., & Harter, J. K. (2003). *Investing in Strengths. In K. S. Cameron, J. E. Dutton, & R. E. Quinn (Eds.), Positive Organizational Scholarship* (pp. 111-121). Berrett-Koehler.
- McGrath, R. E. (2021). Technical issues with the VIA Assessment. *Journal of Positive Psychology, 16*(1), 121-131.
- Peterson, C., & Seligman, M. E. P. (2004). *Character Strengths and Virtues: A Handbook and Classification*. Oxford University Press.

### Psychometrics & Measurement

- Embretson, S. E., & Reise, S. P. (2000). *Item Response Theory for Psychologists*. Lawrence Erlbaum Associates.
- Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. *Advances in Methods and Practices in Psychological Science, 3*(4), 456-465.
- Ziegler, M., Booth, T., & Bensch, D. (2019). Getting entangled in the nomological net: Thoughts on validity and conceptual overlap. *European Journal of Psychological Assessment, 35*(3), 157-161.

### Creativity & Innovation

- Beaty, R. E., Kenett, Y. N., Christensen, A. P., Rosenberg, M. D., Benedek, M., Chen, Q., ... & Silvia, P. J. (2018). Robust prediction of individual creative ability from brain functional connectivity. *Proceedings of the National Academy of Sciences, 115*(5), 1087-1092.
- Benedek, M., Jauk, E., Sommer, M., Arendasy, M., & Neubauer, A. C. (2014). Intelligence, creativity, and cognitive control: The common and differential involvement of executive functions in intelligence and creativity. *Intelligence, 46*, 73-83.

### Motivation & Conscientiousness

- Duckworth, A. L., Peterson, C., Matthews, M. D., & Kelly, D. R. (2007). Grit: Perseverance and passion for long-term goals. *Journal of Personality and Social Psychology, 92*(6), 1087-1101.
- Moffitt, T. E., Arseneault, L., Belsky, D., Dickson, N., Hancox, R. J., Harrington, H., ... & Caspi, A. (2011). A gradient of childhood self-control predicts health, wealth, and public safety. *Proceedings of the National Academy of Sciences, 108*(7), 2693-2698.
- Payne, S. C., Youngcourt, S. S., & Beaubien, J. M. (2007). A meta-analytic examination of the goal orientation nomological net. *Journal of Applied Psychology, 92*(1), 128-150.

### Stability & Resilience

- Bonanno, G. A. (2004). Loss, trauma, and human resilience: Have we underestimated the human capacity to thrive after extremely aversive events? *American Psychologist, 59*(1), 20-28.
- Christian, M. S., Bradley, J. C., Wallace, J. C., & Burke, M. J. (2009). Workplace safety: A meta-analysis of the roles of person and situation factors. *Journal of Applied Psychology, 94*(5), 1103-1127.
- Tangney, J. P., Baumeister, R. F., & Boone, A. L. (2004). High self-control predicts good adjustment, less pathology, better grades, and interpersonal success. *Journal of Personality, 72*(2), 271-324.

### Connection & Relationships

- Emmons, R. A., & McCullough, M. E. (2003). Counting blessings versus burdens: An experimental investigation of gratitude and subjective well-being in daily life. *Journal of Personality and Social Psychology, 84*(2), 377-389.
- McCullough, M. E., Rachal, K. C., Sandage, S. J., Worthington, E. L., Brown, S. W., & Hight, T. L. (2000). Forgiveness, forbearance, and time: The temporal unfolding of transgression-related interpersonal motivations. *Journal of Personality and Social Psychology, 79*(1), 107-114.

### Leadership & Influence

- Bass, B. M. (1985). *Leadership and Performance Beyond Expectations*. Free Press.
- Bateman, T. S., & Crant, J. M. (1993). The proactive component of organizational behavior: A measure and correlates. *Journal of Organizational Behavior, 14*(2), 103-118.
- Judge, T. A., Bono, J. E., Ilies, R., & Gerhardt, M. W. (2002). Personality and leadership: A qualitative and quantitative review. *Journal of Applied Psychology, 87*(4), 765-780.

### AI & NLP (Selected)

- Natural Language Inference models, semantic similarity research, and GPT-based item generation methods referenced throughout technical sections. Full bibliography available in technical supplement.

---

## Appendix F: Budget & Funding Plan

*Available on request to qualified partners and reviewers.*

---

## Document Information

**Prepared by:** The OpenStrengths Working Group

**Version:** 2.0 (Combined Edition)

**Date:** November 2025

**Purpose:** This combined whitepaper integrates audience-specific perspectives (students, business stakeholders, funders, researchers) into a single unified narrative with progressive depth disclosure.

**Updates:** This is a living document. As development progresses and validation results emerge, we will release updated versions with transparent changelog.

**License:** This whitepaper is released under Creative Commons Attribution 4.0 International (CC BY 4.0). You are free to share and adapt with attribution.

---

**Questions? Feedback? Brutal criticism? We want to hear it.**

**Contact:** [To be added]

---

*End of Document*
